{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "from time import sleep\n",
    "import numpy as np \n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix\n",
    "\n",
    "from random import randint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "\n",
    "#Questions:\n",
    "#question: Is there a better way to do the input than just flattening?\n",
    "\n",
    "#Completed tasks\n",
    "#Write a normal neural net to try to predict the next measurement with the window method in multiple dimensions (train_baseline_multi)\n",
    "#Write a deep neural net to try to predict the next measurement with the window method in multiple dimensions (train_baseline_multi)\n",
    "#Write a Random Forest Regressor to try to predict the next measurement with the window method in multiple dimensions (train_forest)\n",
    "#Consolidate the pipeline - shape data / create models / validate models \n",
    "#Sliding window with a RNN to predict the next value (train_window_regression_rnn)\n",
    "#Random Forest Classifier to classify FLAG at the next measurement(30 trees, 30 features)\n",
    "#Neural Network Classifier (1 layer, 512 nodes) \n",
    "#Deep Neural Network Classifier (2 layers, 512 nodes) \n",
    "#Recurrent Neural Network for classification using window\n",
    "#Complete all the quizzes and submit a request for MIMIC\n",
    "#Sanity check for A = B\n",
    "    #If it doesn't work, make sure the rolling/unrolling is correct\n",
    "    #Switch order of patient and timestep\n",
    "    #If it still doesn't work, try only feeding it one previous instead of 20 previous\n",
    "#Completed today\n",
    "#Fix the regression\n",
    "    #Make it so that the matrix predictor gives 0 error\n",
    "    #Make the other regressors make sense\n",
    "    #Make all the regressions work\n",
    "#Make all the classifications work similarly\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Regression\n",
    "    #Write scaffolding for testing variation of len_sequence DONE\n",
    "    #Fix the data generation DONE\n",
    "    #Create the graphs\n",
    "    #Write up Regression where A \\neq B\n",
    "    #Write up Regression where A = B\n",
    "#Writeup: Classification \n",
    "    #Write scaffolding for testing variation of len_sequence \n",
    "    #Create the graphs\n",
    "    #Write up Classification\n",
    "#Sequence to sequence rnn classification\n",
    "    #Get a 1-d version working DONE\n",
    "    #Investigate what happens more closely by reducing the number of people to 2 DONE\n",
    "    #Get a multiple dimension version working DONE\n",
    "    #Go back to the toy example and make it work for larger dimensions\n",
    "    #Bring it back to our synthetic data\n",
    "#Figure out what we do if data is missing\n",
    "#Implement it\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#TODO \n",
    "#Investigate if any of our methods can actually predict in the time period where the flag is being flipped: e.g. the previous window has both 0 and 1\n",
    "#Write a normal neural net to take prediction from rnn and predict 1/0\n",
    "#Investigate this idea of deleting data\n",
    "#Add a stateful try where the rnn is an autoencoder ( http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) and then another LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33980975239 8.33980975239\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "V=np.random.randn(10)\n",
    "print(V.dot(V), np.linalg.norm(V)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup for the generation of the data \n",
    "d=10 ##number of measurements per patient per time unit\n",
    "k=2 ##complexity of the time series\n",
    "n=200 ## number of patients\n",
    "len_sequence = 10\n",
    "sigmaw=0.2##randomness\n",
    "T=175 ## amount of time measured for each patient (also batch size)\n",
    "U=np.random.randn(d,k).dot(np.random.randn(k,d))\n",
    "betashift=np.random.randn(d)/np.sqrt(d)  ## used to check when we switch to the other time series\n",
    "#normalized U\n",
    "A=0.9*U/np.linalg.norm(U,ord=2)\n",
    "#V is also d by d\n",
    "V=np.random.randn(d,k).dot(np.random.randn(k,d))\n",
    "#normalized V\n",
    "B = .9*V/np.linalg.norm(V,ord=2) \n",
    "B = A #Set B = A for sanity checking\n",
    "flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    X = np.zeros((n,T,d)) #this represents n patients, in d-dimensions, for T time-steps\n",
    "    flags = np.zeros((n,T))\n",
    "    #Generate the data, fill in X, flags\n",
    "    for j in range(n): #for each patient\n",
    "        X[j,0,:]=np.random.randn(d)/np.sqrt(d)\n",
    "        flag=1\n",
    "        for i in range(1,T): #for each time      \n",
    "            if flag and ((X[j,i-1,:].dot(betashift))>.5): ##if flag was true and we go above a certain threshold, then set flag=0\n",
    "                flag = 0\n",
    "            ## we can think of flag has flag==1 means not critical, flag==0 means critical\n",
    "            if flag:\n",
    "                X[j,i,:]=A.dot(X[j,i-1,:])+sigmaw*np.random.randn(d)\n",
    "                flags[j,i] = 0\n",
    "            else:\n",
    "                X[j,i,:]=B.dot(X[j,i-1,:])+sigmaw*np.random.randn(d)\n",
    "                flags[j, i] = 1\n",
    "    return(X, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_crazy(len_sequence, X, flags):\n",
    "    ## X is a n by d by T tensorx\n",
    "    ## n represents independent trials (think of them as patients or time periods for patients separated)\n",
    "    ## T represents the time horizon (how long we observe a patient, for now T is the same for everybody, but we can imagine that changing)\n",
    "    ## d is the number of measurements that we have per patient per time.\n",
    "    ## flags are num_patients * num_timesteps * dimensionality\n",
    "    crazy = np.array([X[:, i:i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    crazy_y = np.array([X[:, i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    crazy_flags = np.array([flags[:, i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    return (crazy, crazy_y, crazy_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_sequence = 1\n",
    "(X, flags) = generate_data()\n",
    "(crazy, crazy_y, crazy_flags) = generate_crazy(len_sequence, X, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 175, 10)\n",
      "(200, 175)\n",
      "(174, 200, 1, 10)\n",
      "(174, 200, 10)\n",
      "(174, 200)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(flags.shape)\n",
    "print(crazy.shape)\n",
    "print(crazy_y.shape)\n",
    "print(crazy_flags.shape)\n",
    "\n",
    "#print(flags[6])\n",
    "#print(crazy_flags[:,6])\n",
    "#print(X[0,:,0])\n",
    "#print(crazy[:,0,:,0])\n",
    "#print(crazy_y[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
