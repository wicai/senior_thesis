{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['randint', 'flag']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "from time import sleep\n",
    "import numpy as np \n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix\n",
    "\n",
    "from random import randint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "\n",
    "#Questions:\n",
    "#question: Is there a better way to do the input than just flattening?\n",
    "\n",
    "#Completed tasks\n",
    "#Write a normal neural net to try to predict the next measurement with the window method in multiple dimensions (train_baseline_multi)\n",
    "#Write a deep neural net to try to predict the next measurement with the window method in multiple dimensions (train_baseline_multi)\n",
    "#Write a Random Forest Regressor to try to predict the next measurement with the window method in multiple dimensions (train_forest)\n",
    "#Consolidate the pipeline - shape data / create models / validate models \n",
    "#Sliding window with a RNN to predict the next value (train_window_regression_rnn)\n",
    "#Random Forest Classifier to classify FLAG at the next measurement(30 trees, 30 features)\n",
    "#Neural Network Classifier (1 layer, 512 nodes) \n",
    "#Deep Neural Network Classifier (2 layers, 512 nodes) \n",
    "#Recurrent Neural Network for classification using window\n",
    "#Complete all the quizzes and submit a request for MIMIC\n",
    "#Sanity check for A = B\n",
    "    #If it doesn't work, make sure the rolling/unrolling is correct\n",
    "    #Switch order of patient and timestep\n",
    "    #If it still doesn't work, try only feeding it one previous instead of 20 previous\n",
    "#Completed today\n",
    "#Fix the regression\n",
    "    #Make it so that the matrix predictor gives 0 error\n",
    "    #Make the other regressors make sense\n",
    "    #Make all the regressions work\n",
    "#Make all the classifications work similarly\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Regression\n",
    "    #Write scaffolding for testing variation of len_sequence DONE\n",
    "    #Fix the data generation DONE\n",
    "    #Create the graphs\n",
    "    #Write up Regression where A \\neq B\n",
    "    #Write up Regression where A = B\n",
    "#Writeup: Classification \n",
    "    #Write scaffolding for testing variation of len_sequence \n",
    "    #Create the graphs\n",
    "    #Write up Classification\n",
    "#Sequence to sequence rnn classification\n",
    "    #Get a 1-d version working DONE\n",
    "    #Investigate what happens more closely by reducing the number of people to 2 DONE\n",
    "    #Get a multiple dimension version working DONE\n",
    "    #Go back to the toy example and make it work for larger dimensions\n",
    "    #Bring it back to our synthetic data\n",
    "#Figure out what we do if data is missing\n",
    "#Implement it\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#TODO \n",
    "#Investigate if any of our methods can actually predict in the time period where the flag is being flipped: e.g. the previous window has both 0 and 1\n",
    "#Write a normal neural net to take prediction from rnn and predict 1/0\n",
    "#Investigate this idea of deleting data\n",
    "#Add a stateful try where the rnn is an autoencoder ( http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) and then another LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33980975239 8.33980975239\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "V=np.random.randn(10)\n",
    "print(V.dot(V), np.linalg.norm(V)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup for the generation of the data \n",
    "d=10 ##number of measurements per patient per time unit\n",
    "k=2 ##complexity of the time series\n",
    "n=200 ## number of patients\n",
    "len_sequence = 10\n",
    "sigmaw=0.2##randomness\n",
    "T=175 ## amount of time measured for each patient (also batch size)\n",
    "U=np.random.randn(d,k).dot(np.random.randn(k,d))\n",
    "betashift=np.random.randn(d)/np.sqrt(d)  ## used to check when we switch to the other time series\n",
    "#normalized U\n",
    "A=0.9*U/np.linalg.norm(U,ord=2)\n",
    "#V is also d by d\n",
    "V=np.random.randn(d,k).dot(np.random.randn(k,d))\n",
    "#normalized V\n",
    "B = .9*V/np.linalg.norm(V,ord=2) \n",
    "#B = A #Set B = A for sanity checking\n",
    "flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    X = np.zeros((n,T,d)) #this represents n patients, in d-dimensions, for T time-steps\n",
    "    flags = np.zeros((n,T))\n",
    "    #Generate the data, fill in X, flags\n",
    "    for j in range(n): #for each patient\n",
    "        X[j,0,:]=np.random.randn(d)/np.sqrt(d)\n",
    "        flag=1\n",
    "        for i in range(1,T): #for each time      \n",
    "            if flag and ((X[j,i-1,:].dot(betashift))>.5): ##if flag was true and we go above a certain threshold, then set flag=0\n",
    "                flag = 0\n",
    "            ## we can think of flag has flag==1 means not critical, flag==0 means critical\n",
    "            if flag:\n",
    "                X[j,i,:]=A.dot(X[j,i-1,:])+sigmaw*np.random.randn(d)\n",
    "                flags[j,i] = 0\n",
    "            else:\n",
    "                X[j,i,:]=B.dot(X[j,i-1,:])+sigmaw*np.random.randn(d)\n",
    "                flags[j, i] = 1\n",
    "    return(X, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_crazy(len_sequence, X, flags):\n",
    "    ## X is a n by d by T tensorx\n",
    "    ## n represents independent trials (think of them as patients or time periods for patients separated)\n",
    "    ## T represents the time horizon (how long we observe a patient, for now T is the same for everybody, but we can imagine that changing)\n",
    "    ## d is the number of measurements that we have per patient per time.\n",
    "    ## flags are num_patients * num_timesteps * dimensionality\n",
    "    crazy = np.array([X[:, i:i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    crazy_y = np.array([X[:, i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    crazy_flags = np.array([flags[:, i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    return (crazy, crazy_y, crazy_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 175, 10)\n",
      "(200, 175)\n",
      "(158, 200, 17, 10)\n",
      "(158, 200, 10)\n",
      "(158, 200)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(flags.shape)\n",
    "print(crazy.shape)\n",
    "print(crazy_y.shape)\n",
    "print(crazy_flags.shape)\n",
    "\n",
    "#print(flags[6])\n",
    "#print(crazy_flags[:,6])\n",
    "#print(X[0,:,0])\n",
    "#print(crazy[:,0,:,0])\n",
    "#print(crazy_y[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THIS ONE IS FOR REGRESSION\n",
    "all_models = []\n",
    "all_data = []\n",
    "rnn_data = []\n",
    "(X, flags) = generate_data()\n",
    "\n",
    "for i in range(1,20,4):\n",
    "    len_sequence = i\n",
    "    (crazy, crazy_y, crazy_flags) = generate_crazy(len_sequence, X, flags)\n",
    "    (X_train, y_train, X_test, y_test) = prepare_baseline(crazy, crazy_y)\n",
    "    all_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    models = {}\n",
    "    tree_models = train_forest(X_train, y_train)\n",
    "    models['tree'] = tree_models[0]\n",
    "    models['nn'] = train_baseline_nn(X_train, y_train, X_test, y_test)\n",
    "    models['deep_nn'] = train_baseline_multi_deep(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    (X_train, y_train, X_test, y_test) = prepare_rnn(crazy, crazy_y)\n",
    "    rnn_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    models['window_rnn'] = train_window_regression_rnn(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    all_models.append(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 200, 1, 10)\n",
      "(174, 200)\n",
      "(34800, 10)\n",
      "(34800,)\n",
      "(27840, 10)\n",
      "(27840,)\n",
      "(6960, 10)\n",
      "(6960,)\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 27840 samples, validate on 6960 samples\n",
      "Epoch 1/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5573 - acc: 0.7422 - val_loss: 0.4956 - val_acc: 0.7922\n",
      "Epoch 2/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5363 - acc: 0.7521 - val_loss: 0.4904 - val_acc: 0.7938\n",
      "Epoch 3/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5332 - acc: 0.7528 - val_loss: 0.4917 - val_acc: 0.7918\n",
      "Epoch 4/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5320 - acc: 0.7547 - val_loss: 0.4961 - val_acc: 0.7899\n",
      "Epoch 5/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5311 - acc: 0.7543 - val_loss: 0.4923 - val_acc: 0.7922\n",
      "Epoch 6/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5307 - acc: 0.7541 - val_loss: 0.4914 - val_acc: 0.7937\n",
      "Epoch 7/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5304 - acc: 0.7543 - val_loss: 0.4905 - val_acc: 0.7925\n",
      "Epoch 8/10\n",
      "27840/27840 [==============================] - 3s - loss: 0.5293 - acc: 0.7540 - val_loss: 0.4975 - val_acc: 0.7905\n",
      "Epoch 9/10\n",
      "27840/27840 [==============================] - 3s - loss: 0.5294 - acc: 0.7546 - val_loss: 0.4859 - val_acc: 0.7921\n",
      "Epoch 10/10\n",
      "27840/27840 [==============================] - 3s - loss: 0.5292 - acc: 0.7541 - val_loss: 0.4907 - val_acc: 0.7912\n",
      "Train on 27840 samples, validate on 6960 samples\n",
      "Epoch 1/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5493 - acc: 0.7466 - val_loss: 0.4916 - val_acc: 0.7925\n",
      "Epoch 2/10\n",
      "27840/27840 [==============================] - 7s - loss: 0.5369 - acc: 0.7525 - val_loss: 0.4975 - val_acc: 0.7915\n",
      "Epoch 3/10\n",
      "27840/27840 [==============================] - 7s - loss: 0.5348 - acc: 0.7514 - val_loss: 0.4873 - val_acc: 0.7932\n",
      "Epoch 4/10\n",
      "27840/27840 [==============================] - 7s - loss: 0.5331 - acc: 0.7533 - val_loss: 0.4955 - val_acc: 0.7905\n",
      "Epoch 5/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5323 - acc: 0.7532 - val_loss: 0.4955 - val_acc: 0.7886\n",
      "Epoch 6/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5320 - acc: 0.7536 - val_loss: 0.4923 - val_acc: 0.7918\n",
      "Epoch 7/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5308 - acc: 0.7546 - val_loss: 0.4899 - val_acc: 0.7944\n",
      "Epoch 8/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5303 - acc: 0.7538 - val_loss: 0.4955 - val_acc: 0.7924\n",
      "Epoch 9/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5291 - acc: 0.7551 - val_loss: 0.4921 - val_acc: 0.7938\n",
      "Epoch 10/10\n",
      "27840/27840 [==============================] - 7s - loss: 0.5289 - acc: 0.7550 - val_loss: 0.4901 - val_acc: 0.7945\n",
      "(174, 200, 1, 10)\n",
      "(174, 200)\n",
      "(34800, 1, 10)\n",
      "(34800,)\n",
      "(27840, 1, 10)\n",
      "(27840,)\n",
      "(6960, 1, 10)\n",
      "(6960,)\n",
      "Train on 27840 samples, validate on 6960 samples\n",
      "Epoch 1/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5724 - acc: 0.7398 - val_loss: 0.5076 - val_acc: 0.7878\n",
      "Epoch 2/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5464 - acc: 0.7463 - val_loss: 0.5044 - val_acc: 0.7889\n",
      "Epoch 3/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5389 - acc: 0.7512 - val_loss: 0.4920 - val_acc: 0.7925\n",
      "Epoch 4/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5365 - acc: 0.7527 - val_loss: 0.4988 - val_acc: 0.7878\n",
      "Epoch 5/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5351 - acc: 0.7521 - val_loss: 0.4950 - val_acc: 0.7908\n",
      "Epoch 6/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5343 - acc: 0.7523 - val_loss: 0.4872 - val_acc: 0.7937\n",
      "Epoch 7/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5338 - acc: 0.7516 - val_loss: 0.4935 - val_acc: 0.7917\n",
      "Epoch 8/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5327 - acc: 0.7523 - val_loss: 0.4894 - val_acc: 0.7931\n",
      "Epoch 9/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5314 - acc: 0.7529 - val_loss: 0.4901 - val_acc: 0.7925\n",
      "Epoch 10/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5324 - acc: 0.7536 - val_loss: 0.4879 - val_acc: 0.7921\n",
      "(170, 200, 5, 10)\n",
      "(170, 200)\n",
      "(34000, 50)\n",
      "(34000,)\n",
      "(27200, 50)\n",
      "(27200,)\n",
      "(6800, 50)\n",
      "(6800,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 27200 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.4572 - acc: 0.7830 - val_loss: 0.3163 - val_acc: 0.8763\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.3048 - acc: 0.8731 - val_loss: 0.2613 - val_acc: 0.9009\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.2578 - acc: 0.8952 - val_loss: 0.2405 - val_acc: 0.9104\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.2327 - acc: 0.9084 - val_loss: 0.2261 - val_acc: 0.9128\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.2185 - acc: 0.9138 - val_loss: 0.2195 - val_acc: 0.9160\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.2087 - acc: 0.9192 - val_loss: 0.2196 - val_acc: 0.9126\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.2009 - acc: 0.9193 - val_loss: 0.2135 - val_acc: 0.9181\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.1949 - acc: 0.9233 - val_loss: 0.2197 - val_acc: 0.9165\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.1919 - acc: 0.9246 - val_loss: 0.2204 - val_acc: 0.9141\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.1861 - acc: 0.9281 - val_loss: 0.2191 - val_acc: 0.9168\n",
      "Train on 27200 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.3864 - acc: 0.8266 - val_loss: 0.2486 - val_acc: 0.9015\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.2553 - acc: 0.8933 - val_loss: 0.2408 - val_acc: 0.9024\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.2307 - acc: 0.9075 - val_loss: 0.2171 - val_acc: 0.9151\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.2153 - acc: 0.9131 - val_loss: 0.2156 - val_acc: 0.9151\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.2031 - acc: 0.9188 - val_loss: 0.2197 - val_acc: 0.9146\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.1934 - acc: 0.9230 - val_loss: 0.2180 - val_acc: 0.9178\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.1855 - acc: 0.9262 - val_loss: 0.2206 - val_acc: 0.9165\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.1775 - acc: 0.9292 - val_loss: 0.2251 - val_acc: 0.9131\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.1692 - acc: 0.9328 - val_loss: 0.2289 - val_acc: 0.9149\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.1618 - acc: 0.9380 - val_loss: 0.2479 - val_acc: 0.9071\n",
      "(170, 200, 5, 10)\n",
      "(170, 200)\n",
      "(34000, 5, 10)\n",
      "(34000,)\n",
      "(27200, 5, 10)\n",
      "(27200,)\n",
      "(6800, 5, 10)\n",
      "(6800,)\n",
      "Train on 27200 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5449 - acc: 0.7379 - val_loss: 0.4277 - val_acc: 0.8035\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.4043 - acc: 0.8126 - val_loss: 0.3245 - val_acc: 0.8597\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.3293 - acc: 0.8557 - val_loss: 0.2952 - val_acc: 0.8726\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2940 - acc: 0.8763 - val_loss: 0.2687 - val_acc: 0.8831\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2682 - acc: 0.8881 - val_loss: 0.2498 - val_acc: 0.8981\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2517 - acc: 0.8958 - val_loss: 0.2816 - val_acc: 0.8797\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2435 - acc: 0.9013 - val_loss: 0.2353 - val_acc: 0.9041\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2365 - acc: 0.9043 - val_loss: 0.2586 - val_acc: 0.8918\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2317 - acc: 0.9051 - val_loss: 0.2259 - val_acc: 0.9099\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.2257 - acc: 0.9083 - val_loss: 0.2563 - val_acc: 0.8954\n",
      "(166, 200, 9, 10)\n",
      "(166, 200)\n",
      "(33200, 90)\n",
      "(33200,)\n",
      "(26560, 90)\n",
      "(26560,)\n",
      "(6640, 90)\n",
      "(6640,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 26560 samples, validate on 6640 samples\n",
      "Epoch 1/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.4473 - acc: 0.7922 - val_loss: 0.2724 - val_acc: 0.9083\n",
      "Epoch 2/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.2413 - acc: 0.9123 - val_loss: 0.1965 - val_acc: 0.9343\n",
      "Epoch 3/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1831 - acc: 0.9368 - val_loss: 0.1697 - val_acc: 0.9434\n",
      "Epoch 4/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1544 - acc: 0.9486 - val_loss: 0.1589 - val_acc: 0.9473\n",
      "Epoch 5/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.1362 - acc: 0.9552 - val_loss: 0.1549 - val_acc: 0.9473\n",
      "Epoch 6/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1223 - acc: 0.9607 - val_loss: 0.1542 - val_acc: 0.9476\n",
      "Epoch 7/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1082 - acc: 0.9663 - val_loss: 0.1499 - val_acc: 0.9502\n",
      "Epoch 8/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.0986 - acc: 0.9694 - val_loss: 0.1522 - val_acc: 0.9500\n",
      "Epoch 9/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.0911 - acc: 0.9708 - val_loss: 0.1582 - val_acc: 0.9473\n",
      "Epoch 10/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.0847 - acc: 0.9745 - val_loss: 0.1556 - val_acc: 0.9483\n",
      "Train on 26560 samples, validate on 6640 samples\n",
      "Epoch 1/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.3611 - acc: 0.8417 - val_loss: 0.1943 - val_acc: 0.9309\n",
      "Epoch 2/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.1901 - acc: 0.9297 - val_loss: 0.1756 - val_acc: 0.9364\n",
      "Epoch 3/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.1485 - acc: 0.9451 - val_loss: 0.1560 - val_acc: 0.9450\n",
      "Epoch 4/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.1233 - acc: 0.9559 - val_loss: 0.1621 - val_acc: 0.9450\n",
      "Epoch 5/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.1030 - acc: 0.9634 - val_loss: 0.1612 - val_acc: 0.9461\n",
      "Epoch 6/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.0944 - acc: 0.9671 - val_loss: 0.1641 - val_acc: 0.9450\n",
      "Epoch 7/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.0804 - acc: 0.9716 - val_loss: 0.1743 - val_acc: 0.9434\n",
      "Epoch 8/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.0721 - acc: 0.9738 - val_loss: 0.1782 - val_acc: 0.9441\n",
      "Epoch 9/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.0625 - acc: 0.9769 - val_loss: 0.1896 - val_acc: 0.9458\n",
      "Epoch 10/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.0583 - acc: 0.9786 - val_loss: 0.1907 - val_acc: 0.9449\n",
      "(166, 200, 9, 10)\n",
      "(166, 200)\n",
      "(33200, 9, 10)\n",
      "(33200,)\n",
      "(26560, 9, 10)\n",
      "(26560,)\n",
      "(6640, 9, 10)\n",
      "(6640,)\n",
      "Train on 26560 samples, validate on 6640 samples\n",
      "Epoch 1/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5325 - acc: 0.7512 - val_loss: 0.3117 - val_acc: 0.8834\n",
      "Epoch 2/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.2594 - acc: 0.8979 - val_loss: 0.2029 - val_acc: 0.9261\n",
      "Epoch 3/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.1911 - acc: 0.9278 - val_loss: 0.1848 - val_acc: 0.9325\n",
      "Epoch 4/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.1587 - acc: 0.9412 - val_loss: 0.1630 - val_acc: 0.9432\n",
      "Epoch 5/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.1422 - acc: 0.9502 - val_loss: 0.1594 - val_acc: 0.9447\n",
      "Epoch 6/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1291 - acc: 0.9545 - val_loss: 0.1531 - val_acc: 0.9506\n",
      "Epoch 7/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1194 - acc: 0.9578 - val_loss: 0.1580 - val_acc: 0.9438\n",
      "Epoch 8/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.1121 - acc: 0.9611 - val_loss: 0.1952 - val_acc: 0.9304\n",
      "Epoch 9/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.1070 - acc: 0.9624 - val_loss: 0.1397 - val_acc: 0.9532\n",
      "Epoch 10/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.1011 - acc: 0.9651 - val_loss: 0.1461 - val_acc: 0.9511\n",
      "(162, 200, 13, 10)\n",
      "(162, 200)\n",
      "(32400, 130)\n",
      "(32400,)\n",
      "(25920, 130)\n",
      "(25920,)\n",
      "(6480, 130)\n",
      "(6480,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 25920 samples, validate on 6480 samples\n",
      "Epoch 1/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.4490 - acc: 0.7923 - val_loss: 0.2585 - val_acc: 0.9117\n",
      "Epoch 2/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.2129 - acc: 0.9310 - val_loss: 0.1713 - val_acc: 0.9463\n",
      "Epoch 3/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.1544 - acc: 0.9515 - val_loss: 0.1524 - val_acc: 0.9503\n",
      "Epoch 4/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.1237 - acc: 0.9631 - val_loss: 0.1421 - val_acc: 0.9509\n",
      "Epoch 5/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.1060 - acc: 0.9700 - val_loss: 0.1378 - val_acc: 0.9539\n",
      "Epoch 6/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.0890 - acc: 0.9754 - val_loss: 0.1344 - val_acc: 0.9546\n",
      "Epoch 7/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.0779 - acc: 0.9777 - val_loss: 0.1361 - val_acc: 0.9542\n",
      "Epoch 8/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.0658 - acc: 0.9816 - val_loss: 0.1394 - val_acc: 0.9517\n",
      "Epoch 9/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.0570 - acc: 0.9836 - val_loss: 0.1427 - val_acc: 0.9537\n",
      "Epoch 10/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.0491 - acc: 0.9868 - val_loss: 0.1418 - val_acc: 0.9543\n",
      "Train on 25920 samples, validate on 6480 samples\n",
      "Epoch 1/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.3655 - acc: 0.8388 - val_loss: 0.1752 - val_acc: 0.9377\n",
      "Epoch 2/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.1705 - acc: 0.9410 - val_loss: 0.1420 - val_acc: 0.9508\n",
      "Epoch 3/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.1195 - acc: 0.9597 - val_loss: 0.1374 - val_acc: 0.9509\n",
      "Epoch 4/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0950 - acc: 0.9672 - val_loss: 0.1585 - val_acc: 0.9435\n",
      "Epoch 5/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0754 - acc: 0.9748 - val_loss: 0.1518 - val_acc: 0.9483\n",
      "Epoch 6/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0601 - acc: 0.9799 - val_loss: 0.1528 - val_acc: 0.9495\n",
      "Epoch 7/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0503 - acc: 0.9817 - val_loss: 0.1654 - val_acc: 0.9514\n",
      "Epoch 8/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0463 - acc: 0.9839 - val_loss: 0.1672 - val_acc: 0.9514\n",
      "Epoch 9/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0368 - acc: 0.9865 - val_loss: 0.1810 - val_acc: 0.9500\n",
      "Epoch 10/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.0349 - acc: 0.9873 - val_loss: 0.1926 - val_acc: 0.9508\n",
      "(162, 200, 13, 10)\n",
      "(162, 200)\n",
      "(32400, 13, 10)\n",
      "(32400,)\n",
      "(25920, 13, 10)\n",
      "(25920,)\n",
      "(6480, 13, 10)\n",
      "(6480,)\n",
      "Train on 25920 samples, validate on 6480 samples\n",
      "Epoch 1/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5263 - acc: 0.7481 - val_loss: 0.3325 - val_acc: 0.8937\n",
      "Epoch 2/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.2589 - acc: 0.9085 - val_loss: 0.1808 - val_acc: 0.9383\n",
      "Epoch 3/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.1649 - acc: 0.9410 - val_loss: 0.1570 - val_acc: 0.9457\n",
      "Epoch 4/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.1366 - acc: 0.9529 - val_loss: 0.1472 - val_acc: 0.9512\n",
      "Epoch 5/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.1222 - acc: 0.9591 - val_loss: 0.1670 - val_acc: 0.9438\n",
      "Epoch 6/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.1098 - acc: 0.9627 - val_loss: 0.1687 - val_acc: 0.9435\n",
      "Epoch 7/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.1004 - acc: 0.9657 - val_loss: 0.1386 - val_acc: 0.9546\n",
      "Epoch 8/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.0922 - acc: 0.9694 - val_loss: 0.1346 - val_acc: 0.9585\n",
      "Epoch 9/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.0851 - acc: 0.9709 - val_loss: 0.1327 - val_acc: 0.9576\n",
      "Epoch 10/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.0783 - acc: 0.9736 - val_loss: 0.1275 - val_acc: 0.9591\n",
      "(158, 200, 17, 10)\n",
      "(158, 200)\n",
      "(31600, 170)\n",
      "(31600,)\n",
      "(25280, 170)\n",
      "(25280,)\n",
      "(6320, 170)\n",
      "(6320,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 25280 samples, validate on 6320 samples\n",
      "Epoch 1/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.4648 - acc: 0.7866 - val_loss: 0.2664 - val_acc: 0.9250\n",
      "Epoch 2/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.2098 - acc: 0.9383 - val_loss: 0.1647 - val_acc: 0.9478\n",
      "Epoch 3/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.1422 - acc: 0.9592 - val_loss: 0.1413 - val_acc: 0.9560\n",
      "Epoch 4/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.1097 - acc: 0.9697 - val_loss: 0.1307 - val_acc: 0.9560\n",
      "Epoch 5/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.0862 - acc: 0.9764 - val_loss: 0.1287 - val_acc: 0.9568\n",
      "Epoch 6/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.0700 - acc: 0.9812 - val_loss: 0.1326 - val_acc: 0.9555\n",
      "Epoch 7/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.0572 - acc: 0.9845 - val_loss: 0.1288 - val_acc: 0.9573\n",
      "Epoch 8/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.0452 - acc: 0.9880 - val_loss: 0.1328 - val_acc: 0.9559\n",
      "Epoch 9/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.0367 - acc: 0.9907 - val_loss: 0.1330 - val_acc: 0.9571\n",
      "Epoch 10/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.0299 - acc: 0.9928 - val_loss: 0.1470 - val_acc: 0.9576\n",
      "Train on 25280 samples, validate on 6320 samples\n",
      "Epoch 1/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.3881 - acc: 0.8305 - val_loss: 0.1816 - val_acc: 0.9366\n",
      "Epoch 2/10\n",
      "25280/25280 [==============================] - 9s - loss: 0.1705 - acc: 0.9409 - val_loss: 0.1413 - val_acc: 0.9536\n",
      "Epoch 3/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.1124 - acc: 0.9627 - val_loss: 0.1394 - val_acc: 0.9557\n",
      "Epoch 4/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0810 - acc: 0.9729 - val_loss: 0.1572 - val_acc: 0.9500\n",
      "Epoch 5/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0629 - acc: 0.9785 - val_loss: 0.1460 - val_acc: 0.9568\n",
      "Epoch 6/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0459 - acc: 0.9834 - val_loss: 0.1775 - val_acc: 0.9549\n",
      "Epoch 7/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0381 - acc: 0.9868 - val_loss: 0.1745 - val_acc: 0.9552\n",
      "Epoch 8/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0363 - acc: 0.9862 - val_loss: 0.1678 - val_acc: 0.9551\n",
      "Epoch 9/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0268 - acc: 0.9910 - val_loss: 0.1972 - val_acc: 0.9551\n",
      "Epoch 10/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.0237 - acc: 0.9910 - val_loss: 0.2010 - val_acc: 0.9563\n",
      "(158, 200, 17, 10)\n",
      "(158, 200)\n",
      "(31600, 17, 10)\n",
      "(31600,)\n",
      "(25280, 17, 10)\n",
      "(25280,)\n",
      "(6320, 17, 10)\n",
      "(6320,)\n",
      "Train on 25280 samples, validate on 6320 samples\n",
      "Epoch 1/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5163 - acc: 0.7555 - val_loss: 0.2581 - val_acc: 0.9035\n",
      "Epoch 2/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.1748 - acc: 0.9408 - val_loss: 0.1460 - val_acc: 0.9489\n",
      "Epoch 3/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.1176 - acc: 0.9621 - val_loss: 0.1626 - val_acc: 0.9437\n",
      "Epoch 4/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.0952 - acc: 0.9694 - val_loss: 0.1044 - val_acc: 0.9616\n",
      "Epoch 5/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.0807 - acc: 0.9754 - val_loss: 0.0959 - val_acc: 0.9674\n",
      "Epoch 6/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.0687 - acc: 0.9780 - val_loss: 0.1056 - val_acc: 0.9660\n",
      "Epoch 7/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.0596 - acc: 0.9814 - val_loss: 0.0884 - val_acc: 0.9703\n",
      "Epoch 8/10\n",
      "25280/25280 [==============================] - 4s - loss: 0.0513 - acc: 0.9835 - val_loss: 0.0871 - val_acc: 0.9707\n",
      "Epoch 9/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.0433 - acc: 0.9859 - val_loss: 0.0933 - val_acc: 0.9707\n",
      "Epoch 10/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.0400 - acc: 0.9871 - val_loss: 0.0933 - val_acc: 0.9714\n"
     ]
    }
   ],
   "source": [
    "#THIS ONE IS FOR CLASSIFICATION\n",
    "all_models = []\n",
    "all_data = []\n",
    "rnn_data = []\n",
    "(X, flags) = generate_data()\n",
    "\n",
    "for i in range(1,20,4):\n",
    "    len_sequence = i\n",
    "    (crazy, crazy_y, crazy_flags) = generate_crazy(len_sequence, X, flags)\n",
    "    (X_train, y_train, X_test, y_test) = prepare_classification_baseline(crazy, crazy_flags)\n",
    "    all_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    #(X_train, y_train, X_test, y_test) = prepare_baseline()\n",
    "    models = {}\n",
    "    tree_models = train_classification_forest(X_train, y_train)\n",
    "    models['tree'] = tree_models[0]\n",
    "    models['nn'] = train_window_classification_nn(X_train, y_train, X_test, y_test)\n",
    "    models['deep_nn'] = train_window_classification_deep_nn(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    (X_train, y_train, X_test, y_test) = prepare_classification_rnn(crazy, crazy_flags)\n",
    "    rnn_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    models['window_rnn'] = train_window_classification_rnn(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    all_models.append(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-77-c35a5df6d479>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-c35a5df6d479>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tree for sequence length 1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tree for sequence length 1\n",
    "\n",
    "[[5121  362]\n",
    " [1307  170]]\n",
    "0.760201149425\n",
    "\n",
    "\n",
    "neural net for sequence length 1\n",
    "\n",
    "[[5355  128]\n",
    " [1325  152]]\n",
    "0.791235632184\n",
    "\n",
    "\n",
    "deep neural net for sequence length 1\n",
    "\n",
    "[[5382  101]\n",
    " [1329  148]]\n",
    "0.794540229885\n",
    "\n",
    "\n",
    "window recurrent neural net for sequence length 1\n",
    "\n",
    "[[5363  120]\n",
    " [1327  150]]\n",
    "0.792097701149\n",
    "\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
    "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
    "\n",
    "tree for sequence length 2\n",
    "\n",
    "[[5295   32]\n",
    " [1205  268]]\n",
    "0.818088235294\n",
    "\n",
    "\n",
    "neural net for sequence length 2\n",
    "\n",
    "[[5107  220]\n",
    " [ 346 1127]]\n",
    "0.916764705882\n",
    "\n",
    "\n",
    "deep neural net for sequence length 2\n",
    "\n",
    "[[4985  342]\n",
    " [ 290 1183]]\n",
    "0.907058823529\n",
    "\n",
    "\n",
    "window recurrent neural net for sequence length 2\n",
    "\n",
    "[[4861  466]\n",
    " [ 245 1228]]\n",
    "0.895441176471\n",
    "\n",
    "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
    "\n",
    "tree for sequence length 3\n",
    "\n",
    "[[5156   15]\n",
    " [1292  177]]\n",
    "0.803162650602\n",
    "\n",
    "\n",
    "neural net for sequence length 3\n",
    "\n",
    "[[5035  136]\n",
    " [ 207 1262]]\n",
    "0.948343373494\n",
    "\n",
    "\n",
    "deep neural net for sequence length 3\n",
    "\n",
    "[[4994  177]\n",
    " [ 189 1280]]\n",
    "0.944879518072\n",
    "\n",
    "\n",
    "window recurrent neural net for sequence length 3\n",
    "\n",
    "[[5102   69]\n",
    " [ 256 1213]]\n",
    "0.951054216867\n",
    "\n",
    "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
    "\n",
    "tree for sequence length 4\n",
    "\n",
    "[[4994   21]\n",
    " [1310  155]]\n",
    "0.794598765432\n",
    "\n",
    "\n",
    "neural net for sequence length 4\n",
    "\n",
    "[[4903  112]\n",
    " [ 184 1281]]\n",
    "0.954320987654\n",
    "\n",
    "\n",
    "deep neural net for sequence length 4\n",
    "\n",
    "[[4905  110]\n",
    " [ 209 1256]]\n",
    "0.950771604938\n",
    "\n",
    "\n",
    "window recurrent neural net for sequence length 4\n",
    "\n",
    "[[4942   73]\n",
    " [ 192 1273]]\n",
    "0.959104938272\n",
    "\n",
    "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
    "\n",
    "tree for sequence length 5\n",
    "\n",
    "[[4839   20]\n",
    " [1318  143]]\n",
    "0.788291139241\n",
    "\n",
    "\n",
    "neural net for sequence length 5\n",
    "\n",
    "[[4790   69]\n",
    " [ 199 1262]]\n",
    "0.957594936709\n",
    "\n",
    "\n",
    "deep neural net for sequence length 5\n",
    "\n",
    "[[4773   86]\n",
    " [ 190 1271]]\n",
    "0.956329113924\n",
    "\n",
    "\n",
    "window recurrent neural net for sequence length 5\n",
    "\n",
    "[[4760   99]\n",
    " [  82 1379]]\n",
    "0.971360759494\n",
    "\n",
    "\n",
    "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
    "<matplotlib.figure.Figure at 0x1511369b0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree for sequence length 1\n",
      "\n",
      "[[5121  362]\n",
      " [1307  170]]\n",
      "0.760201149425\n",
      "\n",
      "\n",
      "neural net for sequence length 1\n",
      "\n",
      "[[5355  128]\n",
      " [1325  152]]\n",
      "0.791235632184\n",
      "\n",
      "\n",
      "deep neural net for sequence length 1\n",
      "\n",
      "[[5382  101]\n",
      " [1329  148]]\n",
      "0.794540229885\n",
      "\n",
      "\n",
      "window recurrent neural net for sequence length 1\n",
      "\n",
      "[[5363  120]\n",
      " [1327  150]]\n",
      "0.792097701149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tree for sequence length 2\n",
      "\n",
      "[[5295   32]\n",
      " [1205  268]]\n",
      "0.818088235294\n",
      "\n",
      "\n",
      "neural net for sequence length 2\n",
      "\n",
      "[[5107  220]\n",
      " [ 346 1127]]\n",
      "0.916764705882\n",
      "\n",
      "\n",
      "deep neural net for sequence length 2\n",
      "\n",
      "[[4985  342]\n",
      " [ 290 1183]]\n",
      "0.907058823529\n",
      "\n",
      "\n",
      "window recurrent neural net for sequence length 2\n",
      "\n",
      "[[4861  466]\n",
      " [ 245 1228]]\n",
      "0.895441176471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tree for sequence length 3\n",
      "\n",
      "[[5156   15]\n",
      " [1292  177]]\n",
      "0.803162650602\n",
      "\n",
      "\n",
      "neural net for sequence length 3\n",
      "\n",
      "[[5035  136]\n",
      " [ 207 1262]]\n",
      "0.948343373494\n",
      "\n",
      "\n",
      "deep neural net for sequence length 3\n",
      "\n",
      "[[4994  177]\n",
      " [ 189 1280]]\n",
      "0.944879518072\n",
      "\n",
      "\n",
      "window recurrent neural net for sequence length 3\n",
      "\n",
      "[[5102   69]\n",
      " [ 256 1213]]\n",
      "0.951054216867\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tree for sequence length 4\n",
      "\n",
      "[[4994   21]\n",
      " [1310  155]]\n",
      "0.794598765432\n",
      "\n",
      "\n",
      "neural net for sequence length 4\n",
      "\n",
      "[[4903  112]\n",
      " [ 184 1281]]\n",
      "0.954320987654\n",
      "\n",
      "\n",
      "deep neural net for sequence length 4\n",
      "\n",
      "[[4905  110]\n",
      " [ 209 1256]]\n",
      "0.950771604938\n",
      "\n",
      "\n",
      "window recurrent neural net for sequence length 4\n",
      "\n",
      "[[4942   73]\n",
      " [ 192 1273]]\n",
      "0.959104938272\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tree for sequence length 5\n",
      "\n",
      "[[4839   20]\n",
      " [1318  143]]\n",
      "0.788291139241\n",
      "\n",
      "\n",
      "neural net for sequence length 5\n",
      "\n",
      "[[4790   69]\n",
      " [ 199 1262]]\n",
      "0.957594936709\n",
      "\n",
      "\n",
      "deep neural net for sequence length 5\n",
      "\n",
      "[[4773   86]\n",
      " [ 190 1271]]\n",
      "0.956329113924\n",
      "\n",
      "\n",
      "window recurrent neural net for sequence length 5\n",
      "\n",
      "[[4760   99]\n",
      " [  82 1379]]\n",
      "0.971360759494\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1511369b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this one is for clssification\n",
    "counter = 1\n",
    "for (model, data, rnn_datum) in zip(all_models, all_data, rnn_data):\n",
    "    (X_train, y_train, X_test, y_test) = data\n",
    "    print(\"tree for sequence length \" + str(counter) + '\\n')\n",
    "    analyze_classification(model['tree'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    print(\"neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze_classification(model['nn'], X_test, y_test, rnn = True)\n",
    "    print('\\n')\n",
    "    print(\"deep neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze_classification(model['deep_nn'], X_test, y_test, rnn = True)\n",
    "    print('\\n')\n",
    "    (X_train, y_train, X_test, y_test) = rnn_datum\n",
    "    print(\"window recurrent neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze_classification(model['window_rnn'], X_test, y_test, rnn=True)\n",
    "    print('\\n')    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for (model, data, rnn_datum) in zip(all_models, all_data, rnn_data):\n",
    "    (X_train, y_train, X_test, y_test) = data\n",
    "    print(\"tree for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['tree'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    print(\"neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['nn'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    print(\"deep neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['deep_nn'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    (X_train, y_train, X_test, y_test) = rnn_datum\n",
    "    print(\"window recurrent neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['window_rnn'], X_test, y_test, rnn=True)\n",
    "    print('\\n')    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/IAAANHCAYAAACVZWb+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSauT/A7Q1yliHqifwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1476d93c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Analysis of Classification\n",
    "#all_zero error: 0.168895379401\n",
    "#smart_predictor error: 0.159935136096\n",
    "\n",
    "sequence_lengths = [1,5,9,13,17]\n",
    "trees = [0.760, 0.818, 0.803, 0.794, 0.788 ]\n",
    "nns = [0.791,0.917,0.948,0.954,0.957]\n",
    "deep_nns = [0.794 , 0.907 , 0.945 , 0.951 , 0.956]\n",
    "window_rnns = [ 0.792,0.895,0.951, 0.959, 0.971]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(sequence_lengths, trees, 'r', label='Random Forest')\n",
    "plt.plot(sequence_lengths, nns, 'b', label='Neural Network')\n",
    "plt.plot(sequence_lengths, deep_nns, 'y', label='Deep Neural Network')\n",
    "plt.plot(sequence_lengths, window_rnns, 'g', label='Recurrent Neural Network')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of measurements per training sample')\n",
    "plt.title('Effect of window size on MSE for regression models where A = B')\n",
    "plt.legend()\n",
    "plt.savefig('len_sequence_v_accuracy_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analysis of Regression when A = B\n",
    "#all_zero error: 0.168895379401\n",
    "#smart_predictor error: 0.159935136096\n",
    "\n",
    "sequence_lengths = [1,5,9,13,17]\n",
    "trees = [0.169523201816,0.163457677349,0.163639987636,0.163690879409,0.163651731491 ]\n",
    "nns = [0.160142227167,0.161104080877,0.161564311629,0.161922431239,0.162239372943]\n",
    "deep_nns = [0.160053838298,0.160528898159,0.160786978029,0.160851565687,0.160993068655]\n",
    "window_rnns = [0.160126886333,0.160145655568,0.160189070326,0.160301765207,0.159979021872]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(sequence_lengths, trees, 'r', label='Random Forest')\n",
    "plt.plot(sequence_lengths, nns, 'b', label='Neural Network')\n",
    "plt.plot(sequence_lengths, deep_nns, 'y', label='Deep Neural Network')\n",
    "plt.plot(sequence_lengths, window_rnns, 'g', label='Recurrent Neural Network')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of measurements per training sample')\n",
    "plt.title('Effect of window size on MSE for regression models where A = B')\n",
    "plt.legend()\n",
    "plt.savefig('len_sequence_v_MSE_A=B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analysis of Regression when A \\neq B\n",
    "# Using just Ax_{t-1} as a predictor 0.163159100363\n",
    "# Using just 0 as a predictor: 0.168873889664\n",
    "sequence_lengths = [1,5,9,13,17]\n",
    "trees = [0.172481252728,  0.165948494495, 0.166031396918, 0.16612096304, 0.166453330425]\n",
    "nns = [0.162772912922,0.163784221657, 0.164316503743, 0.164711745367, 0.165270914672]\n",
    "deep_nns = [0.162570283671,0.162980507276, 0.163516877129, 0.163565435967, 0.163733663807]\n",
    "window_rnns = [0.162826821642, .16191162646, 0.161616112766, 0.161263587408, 0.161016470274]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(sequence_lengths, trees, 'r', label='Random Forest')\n",
    "plt.plot(sequence_lengths, nns, 'b', label='Neural Network')\n",
    "plt.plot(sequence_lengths, deep_nns, 'y', label='Deep Neural Network')\n",
    "plt.plot(sequence_lengths, window_rnns, 'g', label='Recurrent Neural Network')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of measurements per training sample')\n",
    "plt.title('Effect of window size on MSE for regression models')\n",
    "plt.legend()\n",
    "plt.savefig('len_sequence_v_MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Given a model that uses the flattened window, this will print its mean squared error and plot it's results versus reality\n",
    "def analyze(model, X_test, y_test, rnn=False):\n",
    "    plt.clf()\n",
    "    results = model.predict(X_test)\n",
    "    manual = np.zeros(y_test.shape)\n",
    "    if not rnn:\n",
    "        for i in range(manual.shape[0]):\n",
    "            manual[i,:] = A.dot(X_test[i,-d:])\n",
    "    else:\n",
    "        for i in range(manual.shape[0]):\n",
    "            manual[i,:] = A.dot(X_test[i,-1,:])\n",
    "    p_num=1\n",
    "    preds = results[p_num*T:(p_num+1)*T,:]\n",
    "    reals = y_test[p_num*T:(p_num+1)*T,:]\n",
    "    #manuals = manual[int(manual.shape[0]*.5):(int(manual.shape[0]*.5) + 10), :]\n",
    "    \n",
    "    plt.plot(preds, color='red')\n",
    "    plt.plot(reals, color='blue')\n",
    "    plt.ylabel('Measurements')\n",
    "    plt.xlabel('Time')\n",
    "    \n",
    "    #plt.title('Regression with Random Forest')\n",
    "    #plt.savefig('tree_regression_projection_a_eq_b')\n",
    "    \n",
    "    #plt.title('Regression with Neural Network')\n",
    "    #plt.savefig('nn_regression_projection_a_eq_b')\n",
    "    \n",
    "    #plt.title('Regression with Deep Neural Network')\n",
    "    #plt.savefig('deep_nn_regression_projection_a_eq_b')\n",
    "    \n",
    "    plt.title('Regression with Recurrent Neural Network')\n",
    "    plt.savefig('window_rnn_regression_projection_a_eq_b')\n",
    "    \n",
    "    #plt.plot(manuals, color='yellow')\n",
    "    \n",
    "    all_z = np.zeros(y_test.shape)\n",
    "    \n",
    "    print(\"all_zero error: \" + str(mean_absolute_error(y_test, all_z)))\n",
    "    print(\"smart_predictor error: \" + str(mean_absolute_error(y_test, manual)))\n",
    "    print(\"Actual Error: \" + str(mean_absolute_error(y_test, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#(X_train, y_train, X_test, y_test) = all_data[1]\n",
    "#analyze(all_models[1]['tree'], X_test, y_test)\n",
    "\n",
    "#(X_train, y_train, X_test, y_test) = all_data[0]\n",
    "#analyze(all_models[0]['nn'], X_test, y_test)\n",
    "#analyze(all_models[0]['deep_nn'], X_test, y_test)\n",
    "\n",
    "#(X_train, y_train, X_test, y_test) = rnn_data[4]\n",
    "#analyze(all_models[4]['window_rnn'], X_test, y_test, rnn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (17, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_baseline(crazy, crazy_y):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_y[:,:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "\n",
    "    unrolled_x = np.zeros((crazy.shape[0]*crazy.shape[1], crazy.shape[2] * crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[3]))\n",
    "\n",
    "    for i in range(crazy.shape[1]): #patients\n",
    "        for j in range(crazy.shape[0]): #time steps\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i,:]            \n",
    "            for l in range(crazy.shape[2]):#len_sequence\n",
    "                for k in range(crazy.shape[3]): #dimensionality\n",
    "                    unrolled_x[i * crazy.shape[0] + j][l*crazy.shape[3] + k] = rolled_x[j,i,l,k]\n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_classification_baseline(crazy, crazy_flags):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_flags[:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "    unrolled_x = np.zeros((crazy.shape[0]*crazy.shape[1], crazy.shape[2] * crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1]))\n",
    "\n",
    "    for i in range(crazy.shape[1]): #patients\n",
    "        for j in range(crazy.shape[0]): #time steps\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i]\n",
    "            for l in range(crazy.shape[2]):#len_sequence\n",
    "                for k in range(crazy.shape[3]): #dimensionality\n",
    "                    unrolled_x[i * crazy.shape[0] + j][l*crazy.shape[3] + k] = rolled_x[j,i,l,k]\n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "    \n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a model that uses the flattened window for classification,this will print its confusion matrix and plot it's results versus reality\n",
    "def analyze_classification(model, X_test, y_test, rnn=False, start = 0):\n",
    "    if rnn:\n",
    "        results = model.predict(X_test)[:,0]\n",
    "        results = np.where(results>.5, 1, 0)\n",
    "    else:\n",
    "        results = model.predict(X_test)\n",
    "    #only plotting the first T because otherwise you can't see anything\n",
    "    blah = T - len_sequence\n",
    "    preds = results[blah * start:blah * (start + 1)]\n",
    "    reals = y_test[blah * start:blah * (start + 1)]\n",
    "    plt.plot(preds, color='red')\n",
    "    plt.plot(reals, color='blue')\n",
    "    confused = confusion_matrix(y_test, results)\n",
    "    print(confused)\n",
    "    correct = confused[0][0] + confused[1][1]\n",
    "    wrong = confused[0][1] + confused[1][0]\n",
    "    perc = (1.0 * correct)/(correct + wrong)\n",
    "    print(perc)\n",
    "    plt.savefig('tree_classification' + str(start))\n",
    "    plt.clf()\n",
    "    #Compare to all zeros\n",
    "    \"\"\"\n",
    "    print(\"Try all zeros\")\n",
    "    zeros = np.zeros(5600)\n",
    "    print(confusion_matrix(y_test, zeros))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4839   20]\n",
      " [1318  143]]\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.788291139241\n",
      "[[4839   20]\n",
      " [1318  143]]\n",
      "0.788291139241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x151c99a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, Y_train, X_test, y_test) = all_data[4]\n",
    "for i in range(20):\n",
    "    analyze_classification(all_models[4]['tree'], X_test, y_test, rnn=False, start = i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_forest(X_train, y_train):\n",
    "    ne = [min(d * len_sequence, 40)]\n",
    "    nf = [min(d * len_sequence, 40)]\n",
    "    models = []\n",
    "    for e in ne:\n",
    "        for f in nf:   \n",
    "            model = RandomForestRegressor(n_estimators=e, max_features = f, verbose=2)\n",
    "            model.fit(X_train, y_train)\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_baseline_nn(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer=rms)\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_baseline_multi_deep(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer=rms)\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_rnn(crazy, crazy_y):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_y[:,:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "\n",
    "    unrolled_x = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[2], crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[3]))\n",
    "\n",
    "    #maybe we should switch the order of these (but also maybe not it depends how batching works)\n",
    "    for i in range(crazy.shape[1]): #patients\n",
    "        for j in range(crazy.shape[0]): #time steps\n",
    "            unrolled_x[i * crazy.shape[0] + j] = rolled_x[j,i,:,:]\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i,:]            \n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "    \n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict the next measurement.  Trained using the window method.  A GRU + a Dense layer\n",
    "def train_window_regression_rnn(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(16,activation='relu', input_dim = d))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(d))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_classification_rnn(crazy, crazy_flags):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_flags[:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "\n",
    "    unrolled_x = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[2], crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1]))\n",
    "\n",
    "    #maybe we should switch the order of these (but also maybe not it depends how batching works)\n",
    "    for i in range(crazy.shape[1]): #time steps\n",
    "        for j in range(crazy.shape[0]): #patients\n",
    "            unrolled_x[i * crazy.shape[0] + j] = rolled_x[j,i,:,:]\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i]            \n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classification_forest(X_train, y_train):\n",
    "    ne = [min(d * len_sequence, 40)]\n",
    "    nf = [min(d * len_sequence, 40)]\n",
    "    models = []\n",
    "    for e in ne:\n",
    "        for f in nf:   \n",
    "            model = RandomForestClassifier(n_estimators=e, max_features = f, verbose=2)\n",
    "            model.fit(X_train, y_train)\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test) = prepare_classification_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = train_classification_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in models: \n",
    "    print (\"num_estimators: \" + str(m.n_estimators))\n",
    "    print (\"Max Features: \" + str(m.max_features))\n",
    "    analyze_classification(m, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_window_classification_nn(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_window_classification_nn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_window_classification_deep_nn(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = train_window_classification_deep_nn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model2.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Predict the next measurement.  Trained using the window method.  A GRU + a Dense layer\n",
    "def train_window_classification_rnn(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(16,activation='relu', input_dim = d))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test) = prepare_classification_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_window_classification_rnn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "updated_flags = flags[:,:,0]\n",
    "print (updated_flags.shape)\n",
    "new_x = np.zeros((X.shape[0] * X.shape[1], X.shape[2],1))\n",
    "for i in range(X.shape[0]):\n",
    "    for ii in range(X.shape[1]):\n",
    "        for iii in range(X.shape[2]):\n",
    "            new_x[i * X.shape[1] + ii][iii][0] = X[i][ii][iii]\n",
    "new_flags = np.zeros((X.shape[0] * X.shape[1], 1))\n",
    "for i in range(updated_flags.shape[0]):\n",
    "    for ii in range(updated_flags.shape[1]):\n",
    "        new_flags[i * new_flags.shape[1] + ii][0] = updated_flags[i][ii]\n",
    "\n",
    "transition = 50\n",
    "X_train = new_x[0: transition * X.shape[1]]\n",
    "X_test = new_x[transition * X.shape[1]:2*transition*X.shape[1]]\n",
    "y_train = new_flags[0:transition*X.shape[1]]\n",
    "y_test = new_flags[transition*X.shape[1]:2*transition*X.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updated_flags[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResetStatesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if self.counter % T == 0:\n",
    "            self.model.reset_states()\n",
    "        self.counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STATEFUL MODEL\n",
    "print('Build STATEFUL model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512,\n",
    "               batch_input_shape=(1, d, 1), return_sequences=False,\n",
    "               stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          callbacks=[ResetStatesCallback()],\n",
    "      batch_size=1, nb_epoch=1,\n",
    "          shuffle=False, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_train, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = probs[:,0]\n",
    "results = np.where(results>.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rando_sum = 0\n",
    "for i in range(8750):\n",
    "    rando_sum += results[i]\n",
    "print(rando_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EVERYTHING BELOW HERE IS NOT WORKING / OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#given train_split, train the model using the first train_split examples\n",
    "#which = 0 for the normal prediction\n",
    "#which = 1 for crazy_flags\n",
    "#which = 2 for oned_flags\n",
    "def train_stage1(train_split, which):\n",
    "    #First model: Using the sliding window\n",
    "    hists = []\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128,activation='relu', input_shape=(crazy.shape[2:])))\n",
    "    if which == 2:\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model.add(Dense(d))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(train_split): # for each patient\n",
    "        this_patient_x = crazy[:,i,:,:]\n",
    "        if which == 0:\n",
    "            this_patient_y = crazy_y[:,i,:]\n",
    "        elif which == 1:\n",
    "            this_patient_y = crazy_flags[:,i,:]\n",
    "        else:\n",
    "            this_patient_y = oned_flags[:,i,0:1]\n",
    "        for ii in range(crazy.shape[0]): #for each window of time sequence\n",
    "            this_patient_x_i = np.array([this_patient_x[ii,:,:]])\n",
    "            if which == 2:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,0:1]])\n",
    "            else:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,:]])\n",
    "            hists.append(model.train_on_batch(this_patient_x_i, this_patient_y_i))\n",
    "        model.reset_states()\n",
    "    return hists, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_stage2(train_split):\n",
    "    #second model: trading a stateful rnn using this batch thing\n",
    "    hists = []\n",
    "    model = Sequential()\n",
    "    \n",
    "    X  # this is our input data, of shape (32, 21, 16)\n",
    "    # we will feed it to our model in sequences of length 10\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, batch_input_shape=(32, 10, 16), stateful=True))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "    # we train the network to predict the 11th timestep given the first 10:\n",
    "    model.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))\n",
    "\n",
    "    # the state of the network has changed. We can feed the follow-up sequences:\n",
    "    model.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))\n",
    "\n",
    "    # let's reset the states of the LSTM layer:\n",
    "    model.reset_states()\n",
    "\n",
    "    # another way to do it in this case:\n",
    "    model.layers[0].reset_states()\n",
    "    \n",
    "    model.add(GRU(128,activation='relu', input_shape=(crazy.shape[2:])))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(train_split): # for each patient\n",
    "        this_patient_x = crazy[:,i,:,:]\n",
    "        if which == 0:\n",
    "            this_patient_y = crazy_y[:,i,:]\n",
    "        elif which == 1:\n",
    "            this_patient_y = crazy_flags[:,i,:]\n",
    "        else:\n",
    "            this_patient_y = oned_flags[:,i,0:1]\n",
    "        for ii in range(crazy.shape[0]): #for each window of time sequence\n",
    "            this_patient_x_i = np.array([this_patient_x[ii,:,:]])\n",
    "            if which == 2:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,0:1]])\n",
    "            else:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,:]])\n",
    "            hists.append(model.train_on_batch(this_patient_x_i, this_patient_y_i))\n",
    "        model.reset_states()\n",
    "    return hists, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oned_flags.shape\n",
    "(hists, model) = train_stage1(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compare the y between them\n",
    "print(oned_flags[:, 0, 0:1].shape)\n",
    "#this is y for normal stuff\n",
    "print(crazy_y[:,i,:][0,:].shape)\n",
    "#this is y for the newest one\n",
    "print((oned_flags[:,0,0:1][0,0:1]).shape)\n",
    "#compare the x between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(crazy[:, patient_num, :, :].shape)\n",
    "print((crazy[:,0,:,:][0,:,:]).shape)\n",
    "\n",
    "crazy[:,patient_num,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluate the model on the last 10 samples\n",
    "#which = 0 for the normal prediction\n",
    "#which = 1 for crazy_flags\n",
    "#which = 2 for oned_flags\n",
    "def evaluate(model, which):\n",
    "    evaluations = []\n",
    "    for i in range(n-10, n): # for each patient not in the training\n",
    "        this_patient_x = crazy[:,i,:,:]\n",
    "        if which == 0:\n",
    "            this_patient_y = crazy_y[:,i,:]\n",
    "        elif which == 1:\n",
    "            this_patient_y = crazy_flags[:,i,:]\n",
    "        else:\n",
    "            this_patient_y = oned_flags[:,i,:]\n",
    "        for ii in range(crazy.shape[0]): #for each window of time sequence\n",
    "            this_patient_x_i = np.array([this_patient_x[ii,:,:]])            \n",
    "            this_patient_y_i = np.array([this_patient_y[ii,:]])\n",
    "            evaluations.append(model.evaluate(this_patient_x_i, this_patient_y_i, verbose=0))\n",
    "        model.reset_states()\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = {}\n",
    "all_evals = {}\n",
    "which = 2\n",
    "for training_amount in range(10, 100, 10):\n",
    "    #Train the model\n",
    "    (hists, model) = train_stage1(training_amount, which)\n",
    "\n",
    "    #Check training results of the data\n",
    "    hist_values = []\n",
    "    for hist in hists:\n",
    "        hist_values.append(hist.item())\n",
    "    #plt.plot(hist_values)\n",
    "    histories[training_amount] = (sum(hist_values)/float(len(hist_values)))\n",
    "\n",
    "    #Check evaluation of the model\n",
    "    evaluations = evaluate(model, which)\n",
    "    #plt.plot(evaluations)\n",
    "    all_evals[training_amount] = (sum(evaluations)/float(len(evaluations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(histories)\n",
    "for key in histories:\n",
    "    print (key)\n",
    "    print (histories[key] - all_evals[key])\n",
    "print(all_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visually check the prediction of the first reading versus what's actually happening\n",
    "#(178, 200, 20, 10)\n",
    "\n",
    "#choose a patient\n",
    "patient_num = randint(101,199)\n",
    "#choose a measuremen\n",
    "measurement = 0\n",
    "\n",
    "#Get what we are aiming for\n",
    "if which == 0:\n",
    "    reality = crazy_y[:, patient_num, measurement]\n",
    "elif which == 1:\n",
    "    reality = crazy_flags[:, patient_num, measurement]\n",
    "elif which == 2:\n",
    "    reality = oned_flags[:, patient_num, 0:1]\n",
    "#Get the predictions\n",
    "model.reset_states()\n",
    "predictions = model.predict(crazy[:,patient_num,:,:])\n",
    "\n",
    "#Plot prediction versus reality\n",
    "plt.plot(predictions[:,0], color='red')\n",
    "plt.plot(reality, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute MSE\n",
    "dif = reality - predictions[:,0]\n",
    "dif = dif ** 2\n",
    "print (\"MSE:\")\n",
    "print (sum(dif)/178)\n",
    "\n",
    "#Plot the difference\n",
    "plt.plot(dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
