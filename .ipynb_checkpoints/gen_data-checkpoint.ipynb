{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['randint', 'flag']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "from time import sleep\n",
    "import numpy as np \n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix\n",
    "\n",
    "from random import randint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "\n",
    "#Questions:\n",
    "#question: Is there a better way to do the input than just flattening?\n",
    "\n",
    "#Completed tasks\n",
    "#Write a normal neural net to try to predict the next measurement with the window method in multiple dimensions (train_baseline_multi)\n",
    "#Write a deep neural net to try to predict the next measurement with the window method in multiple dimensions (train_baseline_multi)\n",
    "#Write a Random Forest Regressor to try to predict the next measurement with the window method in multiple dimensions (train_forest)\n",
    "#Consolidate the pipeline - shape data / create models / validate models \n",
    "#Sliding window with a RNN to predict the next value (train_window_regression_rnn)\n",
    "#Random Forest Classifier to classify FLAG at the next measurement(30 trees, 30 features)\n",
    "#Neural Network Classifier (1 layer, 512 nodes) \n",
    "#Deep Neural Network Classifier (2 layers, 512 nodes) \n",
    "#Recurrent Neural Network for classification using window\n",
    "#Complete all the quizzes and submit a request for MIMIC\n",
    "#Sanity check for A = B\n",
    "    #If it doesn't work, make sure the rolling/unrolling is correct\n",
    "    #Switch order of patient and timestep\n",
    "    #If it still doesn't work, try only feeding it one previous instead of 20 previous\n",
    "#Completed today\n",
    "#Fix the regression\n",
    "    #Make it so that the matrix predictor gives 0 error\n",
    "    #Make the other regressors make sense\n",
    "    #Make all the regressions work\n",
    "#Make all the classifications work similarly\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Regression\n",
    "    #Write scaffolding for testing variation of len_sequence DONE\n",
    "    #Fix the data generation DONE\n",
    "    #Create the graphs\n",
    "    #Write up Regression where A \\neq B\n",
    "    #Write up Regression where A = B\n",
    "#Writeup: Classification \n",
    "    #Write scaffolding for testing variation of len_sequence \n",
    "    #Create the graphs\n",
    "    #Write up Classification\n",
    "#Sequence to sequence rnn classification\n",
    "    #Get a 1-d version working DONE\n",
    "    #Investigate what happens more closely by reducing the number of people to 2 DONE\n",
    "    #Get a multiple dimension version working DONE\n",
    "    #Go back to the toy example and make it work for larger dimensions\n",
    "    #Bring it back to our synthetic data\n",
    "#Figure out what we do if data is missing\n",
    "#Implement it\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#TODO \n",
    "#Investigate if any of our methods can actually predict in the time period where the flag is being flipped: e.g. the previous window has both 0 and 1\n",
    "#Write a normal neural net to take prediction from rnn and predict 1/0\n",
    "#Investigate this idea of deleting data\n",
    "#Add a stateful try where the rnn is an autoencoder ( http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) and then another LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33980975239 8.33980975239\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "V=np.random.randn(10)\n",
    "print(V.dot(V), np.linalg.norm(V)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup for the generation of the data \n",
    "d=10 ##number of measurements per patient per time unit\n",
    "k=2 ##complexity of the time series\n",
    "n=200 ## number of patients\n",
    "len_sequence = 10\n",
    "sigmaw=0.2##randomness\n",
    "T=175 ## amount of time measured for each patient (also batch size)\n",
    "U=np.random.randn(d,k).dot(np.random.randn(k,d))\n",
    "betashift=np.random.randn(d)/np.sqrt(d)  ## used to check when we switch to the other time series\n",
    "#normalized U\n",
    "A=0.9*U/np.linalg.norm(U,ord=2)\n",
    "#V is also d by d\n",
    "V=np.random.randn(d,k).dot(np.random.randn(k,d))\n",
    "#normalized V\n",
    "B = .9*V/np.linalg.norm(V,ord=2) \n",
    "#B = A #Set B = A for sanity checking\n",
    "flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    X = np.zeros((n,T,d)) #this represents n patients, in d-dimensions, for T time-steps\n",
    "    flags = np.zeros((n,T))\n",
    "    #Generate the data, fill in X, flags\n",
    "    for j in range(n): #for each patient\n",
    "        X[j,0,:]=np.random.randn(d)/np.sqrt(d)\n",
    "        flag=1\n",
    "        for i in range(1,T): #for each time      \n",
    "            if flag and ((X[j,i-1,:].dot(betashift))>.5): ##if flag was true and we go above a certain threshold, then set flag=0\n",
    "                flag = 0\n",
    "            ## we can think of flag has flag==1 means not critical, flag==0 means critical\n",
    "            if flag:\n",
    "                X[j,i,:]=A.dot(X[j,i-1,:])+sigmaw*np.random.randn(d)\n",
    "                flags[j,i] = 0\n",
    "            else:\n",
    "                X[j,i,:]=B.dot(X[j,i-1,:])+sigmaw*np.random.randn(d)\n",
    "                flags[j, i] = 1\n",
    "    return(X, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_crazy(len_sequence, X, flags):\n",
    "    ## X is a n by d by T tensorx\n",
    "    ## n represents independent trials (think of them as patients or time periods for patients separated)\n",
    "    ## T represents the time horizon (how long we observe a patient, for now T is the same for everybody, but we can imagine that changing)\n",
    "    ## d is the number of measurements that we have per patient per time.\n",
    "    ## flags are num_patients * num_timesteps * dimensionality\n",
    "    crazy = np.array([X[:, i:i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    crazy_y = np.array([X[:, i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    crazy_flags = np.array([flags[:, i + len_sequence] for i in range(X.shape[1] - len_sequence)])\n",
    "    return (crazy, crazy_y, crazy_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4919a1e892b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrazy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrazy_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrazy_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(flags.shape)\n",
    "print(crazy.shape)\n",
    "print(crazy_y.shape)\n",
    "print(crazy_flags.shape)\n",
    "\n",
    "#print(flags[6])\n",
    "#print(crazy_flags[:,6])\n",
    "#print(X[0,:,0])\n",
    "#print(crazy[:,0,:,0])\n",
    "#print(crazy_y[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THIS ONE IS FOR REGRESSION\n",
    "all_models = []\n",
    "all_data = []\n",
    "rnn_data = []\n",
    "(X, flags) = generate_data()\n",
    "\n",
    "for i in range(1,20,4):\n",
    "    len_sequence = i\n",
    "    (crazy, crazy_y, crazy_flags) = generate_crazy(len_sequence, X, flags)\n",
    "    (X_train, y_train, X_test, y_test) = prepare_baseline(crazy, crazy_y)\n",
    "    all_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    models = {}\n",
    "    tree_models = train_forest(X_train, y_train)\n",
    "    models['tree'] = tree_models[0]\n",
    "    models['nn'] = train_baseline_nn(X_train, y_train, X_test, y_test)\n",
    "    models['deep_nn'] = train_baseline_multi_deep(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    (X_train, y_train, X_test, y_test) = prepare_rnn(crazy, crazy_y)\n",
    "    rnn_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    models['window_rnn'] = train_window_regression_rnn(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    all_models.append(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 200, 1, 10)\n",
      "(174, 200)\n",
      "(34800, 10)\n",
      "(34800,)\n",
      "(27840, 10)\n",
      "(27840,)\n",
      "(6960, 10)\n",
      "(6960,)\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 27840 samples, validate on 6960 samples\n",
      "Epoch 1/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5693 - acc: 0.7456 - val_loss: 0.5813 - val_acc: 0.7332\n",
      "Epoch 2/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5672 - acc: 0.7462 - val_loss: 0.5816 - val_acc: 0.7332\n",
      "Epoch 3/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5666 - acc: 0.7462 - val_loss: 0.5806 - val_acc: 0.7332\n",
      "Epoch 4/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5668 - acc: 0.7462 - val_loss: 0.5805 - val_acc: 0.7332\n",
      "Epoch 5/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5667 - acc: 0.7462 - val_loss: 0.5797 - val_acc: 0.7332\n",
      "Epoch 6/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5659 - acc: 0.7462 - val_loss: 0.5792 - val_acc: 0.7332\n",
      "Epoch 7/10\n",
      "27840/27840 [==============================] - 3s - loss: 0.5658 - acc: 0.7463 - val_loss: 0.5803 - val_acc: 0.7333\n",
      "Epoch 8/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5654 - acc: 0.7465 - val_loss: 0.5796 - val_acc: 0.7332\n",
      "Epoch 9/10\n",
      "27840/27840 [==============================] - 3s - loss: 0.5648 - acc: 0.7469 - val_loss: 0.5798 - val_acc: 0.7338\n",
      "Epoch 10/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5650 - acc: 0.7472 - val_loss: 0.5815 - val_acc: 0.7345\n",
      "Train on 27840 samples, validate on 6960 samples\n",
      "Epoch 1/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5700 - acc: 0.7456 - val_loss: 0.5798 - val_acc: 0.7332\n",
      "Epoch 2/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5683 - acc: 0.7462 - val_loss: 0.5790 - val_acc: 0.7332\n",
      "Epoch 3/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5673 - acc: 0.7462 - val_loss: 0.5797 - val_acc: 0.7332\n",
      "Epoch 4/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5669 - acc: 0.7462 - val_loss: 0.5807 - val_acc: 0.7332\n",
      "Epoch 5/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5661 - acc: 0.7462 - val_loss: 0.5797 - val_acc: 0.7333\n",
      "Epoch 6/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5659 - acc: 0.7468 - val_loss: 0.5792 - val_acc: 0.7345\n",
      "Epoch 7/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5657 - acc: 0.7476 - val_loss: 0.5783 - val_acc: 0.7341\n",
      "Epoch 8/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5649 - acc: 0.7480 - val_loss: 0.5785 - val_acc: 0.7349\n",
      "Epoch 9/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5645 - acc: 0.7483 - val_loss: 0.5803 - val_acc: 0.7339\n",
      "Epoch 10/10\n",
      "27840/27840 [==============================] - 8s - loss: 0.5646 - acc: 0.7486 - val_loss: 0.5796 - val_acc: 0.7352\n",
      "(174, 200, 1, 10)\n",
      "(174, 200)\n",
      "(34800, 1, 10)\n",
      "(34800,)\n",
      "(27840, 1, 10)\n",
      "(27840,)\n",
      "(6960, 1, 10)\n",
      "(6960,)\n",
      "Train on 27840 samples, validate on 6960 samples\n",
      "Epoch 1/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5724 - acc: 0.7460 - val_loss: 0.5796 - val_acc: 0.7332\n",
      "Epoch 2/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5684 - acc: 0.7462 - val_loss: 0.5800 - val_acc: 0.7332\n",
      "Epoch 3/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5683 - acc: 0.7462 - val_loss: 0.5798 - val_acc: 0.7332\n",
      "Epoch 4/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5680 - acc: 0.7462 - val_loss: 0.5799 - val_acc: 0.7332\n",
      "Epoch 5/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5680 - acc: 0.7462 - val_loss: 0.5797 - val_acc: 0.7332\n",
      "Epoch 6/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5674 - acc: 0.7462 - val_loss: 0.5808 - val_acc: 0.7332\n",
      "Epoch 7/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5669 - acc: 0.7462 - val_loss: 0.5796 - val_acc: 0.7332\n",
      "Epoch 8/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5672 - acc: 0.7462 - val_loss: 0.5803 - val_acc: 0.7332\n",
      "Epoch 9/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5669 - acc: 0.7462 - val_loss: 0.5800 - val_acc: 0.7332\n",
      "Epoch 10/10\n",
      "27840/27840 [==============================] - 2s - loss: 0.5667 - acc: 0.7462 - val_loss: 0.5797 - val_acc: 0.7332\n",
      "(170, 200, 5, 10)\n",
      "(170, 200)\n",
      "(34000, 50)\n",
      "(34000,)\n",
      "(27200, 50)\n",
      "(27200,)\n",
      "(6800, 50)\n",
      "(6800,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 27200 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5752 - acc: 0.7406 - val_loss: 0.5881 - val_acc: 0.7281\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5705 - acc: 0.7413 - val_loss: 0.5870 - val_acc: 0.7281\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5674 - acc: 0.7413 - val_loss: 0.5916 - val_acc: 0.7281\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5645 - acc: 0.7413 - val_loss: 0.5922 - val_acc: 0.7281\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5605 - acc: 0.7414 - val_loss: 0.5917 - val_acc: 0.7278\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5583 - acc: 0.7414 - val_loss: 0.5981 - val_acc: 0.7281\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5547 - acc: 0.7416 - val_loss: 0.5977 - val_acc: 0.7282\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5517 - acc: 0.7420 - val_loss: 0.6014 - val_acc: 0.7271\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5491 - acc: 0.7429 - val_loss: 0.6067 - val_acc: 0.7276\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 2s - loss: 0.5460 - acc: 0.7425 - val_loss: 0.6057 - val_acc: 0.7268\n",
      "Train on 27200 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5757 - acc: 0.7410 - val_loss: 0.5879 - val_acc: 0.7281\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5714 - acc: 0.7413 - val_loss: 0.5897 - val_acc: 0.7281\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5687 - acc: 0.7413 - val_loss: 0.5886 - val_acc: 0.7281\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5647 - acc: 0.7413 - val_loss: 0.5941 - val_acc: 0.7281\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5615 - acc: 0.7412 - val_loss: 0.5946 - val_acc: 0.7278\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5567 - acc: 0.7415 - val_loss: 0.6008 - val_acc: 0.7272\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5511 - acc: 0.7424 - val_loss: 0.6037 - val_acc: 0.7269\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5449 - acc: 0.7429 - val_loss: 0.6071 - val_acc: 0.7235\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 8s - loss: 0.5387 - acc: 0.7426 - val_loss: 0.6140 - val_acc: 0.7184\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 11s - loss: 0.5325 - acc: 0.7463 - val_loss: 0.6209 - val_acc: 0.7171\n",
      "(170, 200, 5, 10)\n",
      "(170, 200)\n",
      "(34000, 5, 10)\n",
      "(34000,)\n",
      "(27200, 5, 10)\n",
      "(27200,)\n",
      "(6800, 5, 10)\n",
      "(6800,)\n",
      "Train on 27200 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 4s - loss: 0.5765 - acc: 0.7405 - val_loss: 0.5838 - val_acc: 0.7281\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 4s - loss: 0.5735 - acc: 0.7413 - val_loss: 0.5852 - val_acc: 0.7281\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 4s - loss: 0.5727 - acc: 0.7413 - val_loss: 0.5838 - val_acc: 0.7281\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5715 - acc: 0.7413 - val_loss: 0.5842 - val_acc: 0.7281\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5708 - acc: 0.7413 - val_loss: 0.5848 - val_acc: 0.7281\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5704 - acc: 0.7413 - val_loss: 0.5848 - val_acc: 0.7281\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5697 - acc: 0.7413 - val_loss: 0.5864 - val_acc: 0.7281\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5689 - acc: 0.7413 - val_loss: 0.5875 - val_acc: 0.7281\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5678 - acc: 0.7413 - val_loss: 0.5871 - val_acc: 0.7281\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 3s - loss: 0.5681 - acc: 0.7413 - val_loss: 0.5866 - val_acc: 0.7281\n",
      "(166, 200, 9, 10)\n",
      "(166, 200)\n",
      "(33200, 90)\n",
      "(33200,)\n",
      "(26560, 90)\n",
      "(26560,)\n",
      "(6640, 90)\n",
      "(6640,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 26560 samples, validate on 6640 samples\n",
      "Epoch 1/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.5812 - acc: 0.7357 - val_loss: 0.5932 - val_acc: 0.7233\n",
      "Epoch 2/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.5728 - acc: 0.7362 - val_loss: 0.5918 - val_acc: 0.7233\n",
      "Epoch 3/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.5675 - acc: 0.7362 - val_loss: 0.5928 - val_acc: 0.7233\n",
      "Epoch 4/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.5604 - acc: 0.7365 - val_loss: 0.6017 - val_acc: 0.7235\n",
      "Epoch 5/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.5514 - acc: 0.7372 - val_loss: 0.6108 - val_acc: 0.7232\n",
      "Epoch 6/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.5427 - acc: 0.7399 - val_loss: 0.6145 - val_acc: 0.7215\n",
      "Epoch 7/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.5347 - acc: 0.7427 - val_loss: 0.6233 - val_acc: 0.7208\n",
      "Epoch 8/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.5262 - acc: 0.7456 - val_loss: 0.6270 - val_acc: 0.7166\n",
      "Epoch 9/10\n",
      "26560/26560 [==============================] - 2s - loss: 0.5182 - acc: 0.7483 - val_loss: 0.6311 - val_acc: 0.7137\n",
      "Epoch 10/10\n",
      "26560/26560 [==============================] - 3s - loss: 0.5122 - acc: 0.7531 - val_loss: 0.6369 - val_acc: 0.7095\n",
      "Train on 26560 samples, validate on 6640 samples\n",
      "Epoch 1/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5818 - acc: 0.7360 - val_loss: 0.5928 - val_acc: 0.7233\n",
      "Epoch 2/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5748 - acc: 0.7362 - val_loss: 0.5921 - val_acc: 0.7233\n",
      "Epoch 3/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5681 - acc: 0.7362 - val_loss: 0.5969 - val_acc: 0.7233\n",
      "Epoch 4/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5600 - acc: 0.7366 - val_loss: 0.6010 - val_acc: 0.7230\n",
      "Epoch 5/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5467 - acc: 0.7387 - val_loss: 0.6160 - val_acc: 0.7226\n",
      "Epoch 6/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5338 - acc: 0.7412 - val_loss: 0.6344 - val_acc: 0.7128\n",
      "Epoch 7/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.5194 - acc: 0.7484 - val_loss: 0.6451 - val_acc: 0.6834\n",
      "Epoch 8/10\n",
      "26560/26560 [==============================] - 9s - loss: 0.5062 - acc: 0.7547 - val_loss: 0.6560 - val_acc: 0.6750\n",
      "Epoch 9/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.4888 - acc: 0.7645 - val_loss: 0.6832 - val_acc: 0.6726\n",
      "Epoch 10/10\n",
      "26560/26560 [==============================] - 8s - loss: 0.4727 - acc: 0.7753 - val_loss: 0.7015 - val_acc: 0.6542\n",
      "(166, 200, 9, 10)\n",
      "(166, 200)\n",
      "(33200, 9, 10)\n",
      "(33200,)\n",
      "(26560, 9, 10)\n",
      "(26560,)\n",
      "(6640, 9, 10)\n",
      "(6640,)\n",
      "Train on 26560 samples, validate on 6640 samples\n",
      "Epoch 1/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5818 - acc: 0.7360 - val_loss: 0.5895 - val_acc: 0.7233\n",
      "Epoch 2/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5775 - acc: 0.7362 - val_loss: 0.5888 - val_acc: 0.7233\n",
      "Epoch 3/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5764 - acc: 0.7362 - val_loss: 0.5886 - val_acc: 0.7233\n",
      "Epoch 4/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5743 - acc: 0.7362 - val_loss: 0.5878 - val_acc: 0.7233\n",
      "Epoch 5/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5711 - acc: 0.7362 - val_loss: 0.5923 - val_acc: 0.7233\n",
      "Epoch 6/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5672 - acc: 0.7362 - val_loss: 0.5908 - val_acc: 0.7233\n",
      "Epoch 7/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5633 - acc: 0.7363 - val_loss: 0.5917 - val_acc: 0.7233\n",
      "Epoch 8/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5606 - acc: 0.7363 - val_loss: 0.5961 - val_acc: 0.7230\n",
      "Epoch 9/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5570 - acc: 0.7367 - val_loss: 0.5959 - val_acc: 0.7224\n",
      "Epoch 10/10\n",
      "26560/26560 [==============================] - 4s - loss: 0.5528 - acc: 0.7372 - val_loss: 0.6086 - val_acc: 0.7224\n",
      "(162, 200, 13, 10)\n",
      "(162, 200)\n",
      "(32400, 130)\n",
      "(32400,)\n",
      "(25920, 130)\n",
      "(25920,)\n",
      "(6480, 130)\n",
      "(6480,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 25920 samples, validate on 6480 samples\n",
      "Epoch 1/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5860 - acc: 0.7310 - val_loss: 0.5999 - val_acc: 0.7184\n",
      "Epoch 2/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5750 - acc: 0.7311 - val_loss: 0.5989 - val_acc: 0.7184\n",
      "Epoch 3/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5668 - acc: 0.7312 - val_loss: 0.6036 - val_acc: 0.7184\n",
      "Epoch 4/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5550 - acc: 0.7323 - val_loss: 0.6059 - val_acc: 0.7171\n",
      "Epoch 5/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5397 - acc: 0.7371 - val_loss: 0.6157 - val_acc: 0.7147\n",
      "Epoch 6/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5223 - acc: 0.7451 - val_loss: 0.6350 - val_acc: 0.7133\n",
      "Epoch 7/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.5065 - acc: 0.7535 - val_loss: 0.6402 - val_acc: 0.7006\n",
      "Epoch 8/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.4898 - acc: 0.7640 - val_loss: 0.6587 - val_acc: 0.7051\n",
      "Epoch 9/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.4754 - acc: 0.7754 - val_loss: 0.6754 - val_acc: 0.6867\n",
      "Epoch 10/10\n",
      "25920/25920 [==============================] - 3s - loss: 0.4615 - acc: 0.7826 - val_loss: 0.6902 - val_acc: 0.6875\n",
      "Train on 25920 samples, validate on 6480 samples\n",
      "Epoch 1/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.5870 - acc: 0.7309 - val_loss: 0.5966 - val_acc: 0.7184\n",
      "Epoch 2/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.5776 - acc: 0.7311 - val_loss: 0.5969 - val_acc: 0.7184\n",
      "Epoch 3/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.5687 - acc: 0.7312 - val_loss: 0.6025 - val_acc: 0.7184\n",
      "Epoch 4/10\n",
      "25920/25920 [==============================] - 9s - loss: 0.5525 - acc: 0.7339 - val_loss: 0.6167 - val_acc: 0.7170\n",
      "Epoch 5/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.5319 - acc: 0.7405 - val_loss: 0.6573 - val_acc: 0.7151\n",
      "Epoch 6/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.5032 - acc: 0.7561 - val_loss: 0.6750 - val_acc: 0.6520\n",
      "Epoch 7/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.4791 - acc: 0.7671 - val_loss: 0.7001 - val_acc: 0.6602\n",
      "Epoch 8/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.4546 - acc: 0.7847 - val_loss: 0.7262 - val_acc: 0.6218\n",
      "Epoch 9/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.4282 - acc: 0.8027 - val_loss: 0.7802 - val_acc: 0.6502\n",
      "Epoch 10/10\n",
      "25920/25920 [==============================] - 8s - loss: 0.4040 - acc: 0.8140 - val_loss: 0.8298 - val_acc: 0.6043\n",
      "(162, 200, 13, 10)\n",
      "(162, 200)\n",
      "(32400, 13, 10)\n",
      "(32400,)\n",
      "(25920, 13, 10)\n",
      "(25920,)\n",
      "(6480, 13, 10)\n",
      "(6480,)\n",
      "Train on 25920 samples, validate on 6480 samples\n",
      "Epoch 1/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5870 - acc: 0.7298 - val_loss: 0.5960 - val_acc: 0.7184\n",
      "Epoch 2/10\n",
      "25920/25920 [==============================] - 5s - loss: 0.5827 - acc: 0.7311 - val_loss: 0.5950 - val_acc: 0.7184\n",
      "Epoch 3/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5804 - acc: 0.7311 - val_loss: 0.5962 - val_acc: 0.7184\n",
      "Epoch 4/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5758 - acc: 0.7311 - val_loss: 0.5962 - val_acc: 0.7184\n",
      "Epoch 5/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5701 - acc: 0.7311 - val_loss: 0.5981 - val_acc: 0.7184\n",
      "Epoch 6/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5641 - acc: 0.7311 - val_loss: 0.6019 - val_acc: 0.7184\n",
      "Epoch 7/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5597 - acc: 0.7306 - val_loss: 0.6027 - val_acc: 0.7179\n",
      "Epoch 8/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5544 - acc: 0.7309 - val_loss: 0.6093 - val_acc: 0.7177\n",
      "Epoch 9/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5496 - acc: 0.7318 - val_loss: 0.6205 - val_acc: 0.7151\n",
      "Epoch 10/10\n",
      "25920/25920 [==============================] - 4s - loss: 0.5438 - acc: 0.7328 - val_loss: 0.6280 - val_acc: 0.7187\n",
      "(158, 200, 17, 10)\n",
      "(158, 200)\n",
      "(31600, 170)\n",
      "(31600,)\n",
      "(25280, 170)\n",
      "(25280,)\n",
      "(6320, 170)\n",
      "(6320,)\n",
      "building tree 1 of 40\n",
      "building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "building tree 4 of 40\n",
      "building tree 5 of 40\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40\n",
      "building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "building tree 10 of 40\n",
      "building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n",
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 25280 samples, validate on 6320 samples\n",
      "Epoch 1/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.5918 - acc: 0.7259 - val_loss: 0.6013 - val_acc: 0.7131\n",
      "Epoch 2/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.5774 - acc: 0.7260 - val_loss: 0.6042 - val_acc: 0.7131\n",
      "Epoch 3/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.5653 - acc: 0.7265 - val_loss: 0.6107 - val_acc: 0.7127\n",
      "Epoch 4/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.5489 - acc: 0.7304 - val_loss: 0.6187 - val_acc: 0.7100\n",
      "Epoch 5/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.5242 - acc: 0.7434 - val_loss: 0.6350 - val_acc: 0.6975\n",
      "Epoch 6/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.4993 - acc: 0.7573 - val_loss: 0.6576 - val_acc: 0.6818\n",
      "Epoch 7/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.4734 - acc: 0.7737 - val_loss: 0.6691 - val_acc: 0.6843\n",
      "Epoch 8/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.4487 - acc: 0.7939 - val_loss: 0.7085 - val_acc: 0.6691\n",
      "Epoch 9/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.4237 - acc: 0.8065 - val_loss: 0.7282 - val_acc: 0.6687\n",
      "Epoch 10/10\n",
      "25280/25280 [==============================] - 3s - loss: 0.4012 - acc: 0.8196 - val_loss: 0.7595 - val_acc: 0.6573\n",
      "Train on 25280 samples, validate on 6320 samples\n",
      "Epoch 1/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.5922 - acc: 0.7259 - val_loss: 0.6004 - val_acc: 0.7131\n",
      "Epoch 2/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.5801 - acc: 0.7259 - val_loss: 0.6042 - val_acc: 0.7131\n",
      "Epoch 3/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.5657 - acc: 0.7263 - val_loss: 0.6127 - val_acc: 0.7131\n",
      "Epoch 4/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.5409 - acc: 0.7352 - val_loss: 0.6453 - val_acc: 0.6693\n",
      "Epoch 5/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.5051 - acc: 0.7536 - val_loss: 0.6750 - val_acc: 0.6266\n",
      "Epoch 6/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.4688 - acc: 0.7757 - val_loss: 0.7169 - val_acc: 0.6546\n",
      "Epoch 7/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.4362 - acc: 0.7936 - val_loss: 0.7734 - val_acc: 0.6636\n",
      "Epoch 8/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.3977 - acc: 0.8191 - val_loss: 0.8400 - val_acc: 0.6163\n",
      "Epoch 9/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.3660 - acc: 0.8352 - val_loss: 0.8855 - val_acc: 0.6225\n",
      "Epoch 10/10\n",
      "25280/25280 [==============================] - 8s - loss: 0.3306 - acc: 0.8538 - val_loss: 0.9470 - val_acc: 0.6176\n",
      "(158, 200, 17, 10)\n",
      "(158, 200)\n",
      "(31600, 17, 10)\n",
      "(31600,)\n",
      "(25280, 17, 10)\n",
      "(25280,)\n",
      "(6320, 17, 10)\n",
      "(6320,)\n",
      "Train on 25280 samples, validate on 6320 samples\n",
      "Epoch 1/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5934 - acc: 0.7240 - val_loss: 0.6000 - val_acc: 0.7131\n",
      "Epoch 2/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5895 - acc: 0.7260 - val_loss: 0.6031 - val_acc: 0.7131\n",
      "Epoch 3/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5888 - acc: 0.7260 - val_loss: 0.6038 - val_acc: 0.7131\n",
      "Epoch 4/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5867 - acc: 0.7260 - val_loss: 0.6004 - val_acc: 0.7131\n",
      "Epoch 5/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5859 - acc: 0.7260 - val_loss: 0.6000 - val_acc: 0.7131\n",
      "Epoch 6/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5834 - acc: 0.7260 - val_loss: 0.5992 - val_acc: 0.7131\n",
      "Epoch 7/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5752 - acc: 0.7260 - val_loss: 0.6260 - val_acc: 0.7131\n",
      "Epoch 8/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5671 - acc: 0.7260 - val_loss: 0.6166 - val_acc: 0.7131\n",
      "Epoch 9/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5572 - acc: 0.7263 - val_loss: 0.6579 - val_acc: 0.7130\n",
      "Epoch 10/10\n",
      "25280/25280 [==============================] - 5s - loss: 0.5466 - acc: 0.7269 - val_loss: 0.6409 - val_acc: 0.7133\n"
     ]
    }
   ],
   "source": [
    "#THIS ONE IS FOR CLASSIFICATION\n",
    "all_models = []\n",
    "all_data = []\n",
    "rnn_data = []\n",
    "(X, flags) = generate_data()\n",
    "\n",
    "for i in range(1,20,4):\n",
    "    len_sequence = i\n",
    "    (crazy, crazy_y, crazy_flags) = generate_crazy(len_sequence, X, flags)\n",
    "    (X_train, y_train, X_test, y_test) = prepare_classification_baseline(crazy, crazy_flags)\n",
    "    all_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    #(X_train, y_train, X_test, y_test) = prepare_baseline()\n",
    "    models = {}\n",
    "    tree_models = train_classification_forest(X_train, y_train)\n",
    "    models['tree'] = tree_models[0]\n",
    "    models['nn'] = train_window_classification_nn(X_train, y_train, X_test, y_test)\n",
    "    models['deep_nn'] = train_window_classification_deep_nn(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    (X_train, y_train, X_test, y_test) = prepare_classification_rnn(crazy, crazy_flags)\n",
    "    rnn_data.append((X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    models['window_rnn'] = train_window_classification_rnn(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    all_models.append(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for (model, data, rnn_datum) in zip(all_models, all_data, rnn_data):\n",
    "    (X_train, y_train, X_test, y_test) = data\n",
    "    print(\"tree for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['tree'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    print(\"neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['nn'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    print(\"deep neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['deep_nn'], X_test, y_test)\n",
    "    print('\\n')\n",
    "    (X_train, y_train, X_test, y_test) = rnn_datum\n",
    "    print(\"window recurrent neural net for sequence length \" + str(counter) + '\\n')\n",
    "    analyze(model['window_rnn'], X_test, y_test, rnn=True)\n",
    "    print('\\n')    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analysis of Regression when A = B\n",
    "#all_zero error: 0.168895379401\n",
    "#smart_predictor error: 0.159935136096\n",
    "\n",
    "sequence_lengths = [1,5,9,13,17]\n",
    "trees = [0.169523201816,0.163457677349,0.163639987636,0.163690879409,0.163651731491 ]\n",
    "nns = [0.160142227167,0.161104080877,0.161564311629,0.161922431239,0.162239372943]\n",
    "deep_nns = [0.160053838298,0.160528898159,0.160786978029,0.160851565687,0.160993068655]\n",
    "window_rnns = [0.160126886333,0.160145655568,0.160189070326,0.160301765207,0.159979021872]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(sequence_lengths, trees, 'r', label='Random Forest')\n",
    "plt.plot(sequence_lengths, nns, 'b', label='Neural Network')\n",
    "plt.plot(sequence_lengths, deep_nns, 'y', label='Deep Neural Network')\n",
    "plt.plot(sequence_lengths, window_rnns, 'g', label='Recurrent Neural Network')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of measurements per training sample')\n",
    "plt.title('Effect of window size on MSE for regression models where A = B')\n",
    "plt.legend()\n",
    "plt.savefig('len_sequence_v_MSE_A=B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analysis of Regression when A \\neq B\n",
    "# Using just Ax_{t-1} as a predictor 0.163159100363\n",
    "# Using just 0 as a predictor: 0.168873889664\n",
    "sequence_lengths = [1,5,9,13,17]\n",
    "trees = [0.172481252728,  0.165948494495, 0.166031396918, 0.16612096304, 0.166453330425]\n",
    "nns = [0.162772912922,0.163784221657, 0.164316503743, 0.164711745367, 0.165270914672]\n",
    "deep_nns = [0.162570283671,0.162980507276, 0.163516877129, 0.163565435967, 0.163733663807]\n",
    "window_rnns = [0.162826821642, .16191162646, 0.161616112766, 0.161263587408, 0.161016470274]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(sequence_lengths, trees, 'r', label='Random Forest')\n",
    "plt.plot(sequence_lengths, nns, 'b', label='Neural Network')\n",
    "plt.plot(sequence_lengths, deep_nns, 'y', label='Deep Neural Network')\n",
    "plt.plot(sequence_lengths, window_rnns, 'g', label='Recurrent Neural Network')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Number of measurements per training sample')\n",
    "plt.title('Effect of window size on MSE for regression models')\n",
    "plt.legend()\n",
    "plt.savefig('len_sequence_v_MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Given a model that uses the flattened window, this will print its mean squared error and plot it's results versus reality\n",
    "def analyze(model, X_test, y_test, rnn=False):\n",
    "    plt.clf()\n",
    "    results = model.predict(X_test)\n",
    "    manual = np.zeros(y_test.shape)\n",
    "    if not rnn:\n",
    "        for i in range(manual.shape[0]):\n",
    "            manual[i,:] = A.dot(X_test[i,-d:])\n",
    "    else:\n",
    "        for i in range(manual.shape[0]):\n",
    "            manual[i,:] = A.dot(X_test[i,-1,:])\n",
    "    p_num=1\n",
    "    preds = results[p_num*T:(p_num+1)*T,:]\n",
    "    reals = y_test[p_num*T:(p_num+1)*T,:]\n",
    "    #manuals = manual[int(manual.shape[0]*.5):(int(manual.shape[0]*.5) + 10), :]\n",
    "    \n",
    "    plt.plot(preds, color='red')\n",
    "    plt.plot(reals, color='blue')\n",
    "    plt.ylabel('Measurements')\n",
    "    plt.xlabel('Time')\n",
    "    \n",
    "    #plt.title('Regression with Random Forest')\n",
    "    #plt.savefig('tree_regression_projection_a_eq_b')\n",
    "    \n",
    "    #plt.title('Regression with Neural Network')\n",
    "    #plt.savefig('nn_regression_projection_a_eq_b')\n",
    "    \n",
    "    #plt.title('Regression with Deep Neural Network')\n",
    "    #plt.savefig('deep_nn_regression_projection_a_eq_b')\n",
    "    \n",
    "    plt.title('Regression with Recurrent Neural Network')\n",
    "    plt.savefig('window_rnn_regression_projection_a_eq_b')\n",
    "    \n",
    "    #plt.plot(manuals, color='yellow')\n",
    "    \n",
    "    all_z = np.zeros(y_test.shape)\n",
    "    \n",
    "    print(\"all_zero error: \" + str(mean_absolute_error(y_test, all_z)))\n",
    "    print(\"smart_predictor error: \" + str(mean_absolute_error(y_test, manual)))\n",
    "    print(\"Actual Error: \" + str(mean_absolute_error(y_test, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#(X_train, y_train, X_test, y_test) = all_data[1]\n",
    "#analyze(all_models[1]['tree'], X_test, y_test)\n",
    "\n",
    "#(X_train, y_train, X_test, y_test) = all_data[0]\n",
    "#analyze(all_models[0]['nn'], X_test, y_test)\n",
    "#analyze(all_models[0]['deep_nn'], X_test, y_test)\n",
    "\n",
    "#(X_train, y_train, X_test, y_test) = rnn_data[4]\n",
    "#analyze(all_models[4]['window_rnn'], X_test, y_test, rnn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (17, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_baseline(crazy, crazy_y):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_y[:,:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "\n",
    "    unrolled_x = np.zeros((crazy.shape[0]*crazy.shape[1], crazy.shape[2] * crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[3]))\n",
    "\n",
    "    for i in range(crazy.shape[1]): #patients\n",
    "        for j in range(crazy.shape[0]): #time steps\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i,:]            \n",
    "            for l in range(crazy.shape[2]):#len_sequence\n",
    "                for k in range(crazy.shape[3]): #dimensionality\n",
    "                    unrolled_x[i * crazy.shape[0] + j][l*crazy.shape[3] + k] = rolled_x[j,i,l,k]\n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_classification_baseline(crazy, crazy_flags):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_flags[:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "    unrolled_x = np.zeros((crazy.shape[0]*crazy.shape[1], crazy.shape[2] * crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1]))\n",
    "\n",
    "    for i in range(crazy.shape[1]): #patients\n",
    "        for j in range(crazy.shape[0]): #time steps\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i]\n",
    "            for l in range(crazy.shape[2]):#len_sequence\n",
    "                for k in range(crazy.shape[3]): #dimensionality\n",
    "                    unrolled_x[i * crazy.shape[0] + j][l*crazy.shape[3] + k] = rolled_x[j,i,l,k]\n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "    \n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a model that uses the flattened window for classification,this will print its confusion matrix and plot it's results versus reality\n",
    "def analyze_classification(model, X_test, y_test):\n",
    "    results = model.predict(X_test)\n",
    "    #only plotting the first 400 because otherwise you can't see anything\n",
    "    preds = results[:400]\n",
    "    reals = y_test[:400]\n",
    "    plt.plot(preds, color='red')\n",
    "    plt.plot(reals, color='blue')\n",
    "    print(confusion_matrix(y_test, results))\n",
    "    #Compare to all zeros\n",
    "    \"\"\"\n",
    "    print(\"Try all zeros\")\n",
    "    zeros = np.zeros(5600)\n",
    "    print(confusion_matrix(y_test, zeros))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_forest(X_train, y_train):\n",
    "    ne = [min(d * len_sequence, 40)]\n",
    "    nf = [min(d * len_sequence, 40)]\n",
    "    models = []\n",
    "    for e in ne:\n",
    "        for f in nf:   \n",
    "            model = RandomForestRegressor(n_estimators=e, max_features = f, verbose=2)\n",
    "            model.fit(X_train, y_train)\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_baseline_nn(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer=rms)\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_baseline_multi_deep(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer=rms)\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_rnn(crazy, crazy_y):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_y[:,:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "\n",
    "    unrolled_x = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[2], crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[3]))\n",
    "\n",
    "    #maybe we should switch the order of these (but also maybe not it depends how batching works)\n",
    "    for i in range(crazy.shape[1]): #patients\n",
    "        for j in range(crazy.shape[0]): #time steps\n",
    "            unrolled_x[i * crazy.shape[0] + j] = rolled_x[j,i,:,:]\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i,:]            \n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "    \n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict the next measurement.  Trained using the window method.  A GRU + a Dense layer\n",
    "def train_window_regression_rnn(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(16,activation='relu', input_dim = d))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(d))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_classification_rnn(crazy, crazy_flags):\n",
    "    #Reshape data and flatten for baseline models\n",
    "    rolled_x = crazy[:,:,:,:]\n",
    "    rolled_y = crazy_flags[:,:]\n",
    "\n",
    "    print (rolled_x.shape)\n",
    "    print (rolled_y.shape)\n",
    "\n",
    "    unrolled_x = np.zeros((crazy.shape[0] * crazy.shape[1], crazy.shape[2], crazy.shape[3]))\n",
    "    unrolled_y = np.zeros((crazy.shape[0] * crazy.shape[1]))\n",
    "\n",
    "    #maybe we should switch the order of these (but also maybe not it depends how batching works)\n",
    "    for i in range(crazy.shape[1]): #time steps\n",
    "        for j in range(crazy.shape[0]): #patients\n",
    "            unrolled_x[i * crazy.shape[0] + j] = rolled_x[j,i,:,:]\n",
    "            unrolled_y[i * crazy.shape[0] + j] = rolled_y[j,i]            \n",
    "\n",
    "    print (unrolled_x.shape)\n",
    "    print (unrolled_y.shape)\n",
    "    transition_point = int(unrolled_x.shape[0] * .8)\n",
    "    \n",
    "    X_train = np.array(unrolled_x[:transition_point,:])\n",
    "    y_train = np.array(unrolled_y[:transition_point])\n",
    "    X_test = np.array(unrolled_x[transition_point:,:])\n",
    "    y_test = np.array(unrolled_y[transition_point:])\n",
    "\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print (X_test.shape)\n",
    "    print (y_test.shape)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classification_forest(X_train, y_train):\n",
    "    ne = [min(d * len_sequence, 40)]\n",
    "    nf = [min(d * len_sequence, 40)]\n",
    "    models = []\n",
    "    for e in ne:\n",
    "        for f in nf:   \n",
    "            model = RandomForestClassifier(n_estimators=e, max_features = f, verbose=2)\n",
    "            model.fit(X_train, y_train)\n",
    "            models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test) = prepare_classification_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = train_classification_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in models: \n",
    "    print (\"num_estimators: \" + str(m.n_estimators))\n",
    "    print (\"Max Features: \" + str(m.max_features))\n",
    "    analyze_classification(m, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_window_classification_nn(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_window_classification_nn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_window_classification_deep_nn(X_train, y_train, X_test, y_test):\n",
    "    #Baseline model in multiple dimensions: predicts the next measurement, all 10 dimensions\n",
    "    rms = RMSprop()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = train_window_classification_deep_nn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model2.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Predict the next measurement.  Trained using the window method.  A GRU + a Dense layer\n",
    "def train_window_classification_rnn(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(16,activation='relu', input_dim = d))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=32, nb_epoch=10, verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test) = prepare_classification_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_window_classification_rnn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "updated_flags = flags[:,:,0]\n",
    "print (updated_flags.shape)\n",
    "new_x = np.zeros((X.shape[0] * X.shape[1], X.shape[2],1))\n",
    "for i in range(X.shape[0]):\n",
    "    for ii in range(X.shape[1]):\n",
    "        for iii in range(X.shape[2]):\n",
    "            new_x[i * X.shape[1] + ii][iii][0] = X[i][ii][iii]\n",
    "new_flags = np.zeros((X.shape[0] * X.shape[1], 1))\n",
    "for i in range(updated_flags.shape[0]):\n",
    "    for ii in range(updated_flags.shape[1]):\n",
    "        new_flags[i * new_flags.shape[1] + ii][0] = updated_flags[i][ii]\n",
    "\n",
    "transition = 50\n",
    "X_train = new_x[0: transition * X.shape[1]]\n",
    "X_test = new_x[transition * X.shape[1]:2*transition*X.shape[1]]\n",
    "y_train = new_flags[0:transition*X.shape[1]]\n",
    "y_test = new_flags[transition*X.shape[1]:2*transition*X.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updated_flags[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResetStatesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if self.counter % T == 0:\n",
    "            self.model.reset_states()\n",
    "        self.counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STATEFUL MODEL\n",
    "print('Build STATEFUL model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512,\n",
    "               batch_input_shape=(1, d, 1), return_sequences=False,\n",
    "               stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          callbacks=[ResetStatesCallback()],\n",
    "      batch_size=1, nb_epoch=1,\n",
    "          shuffle=False, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_train, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = probs[:,0]\n",
    "results = np.where(results>.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rando_sum = 0\n",
    "for i in range(8750):\n",
    "    rando_sum += results[i]\n",
    "print(rando_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(X_test)[:,0]\n",
    "results = np.where(results>.5, 1, 0)\n",
    "print (results)\n",
    "#only plotting the first 400 because otherwise you can't see anything\n",
    "preds = results[:400]\n",
    "reals = y_test[:400]\n",
    "\n",
    "plt.plot(preds, color='red')\n",
    "plt.plot(reals, color='blue')\n",
    "print(confusion_matrix(y_test, results))\n",
    "#Compare to all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EVERYTHING BELOW HERE IS NOT WORKING / OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#given train_split, train the model using the first train_split examples\n",
    "#which = 0 for the normal prediction\n",
    "#which = 1 for crazy_flags\n",
    "#which = 2 for oned_flags\n",
    "def train_stage1(train_split, which):\n",
    "    #First model: Using the sliding window\n",
    "    hists = []\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128,activation='relu', input_shape=(crazy.shape[2:])))\n",
    "    if which == 2:\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model.add(Dense(d))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(train_split): # for each patient\n",
    "        this_patient_x = crazy[:,i,:,:]\n",
    "        if which == 0:\n",
    "            this_patient_y = crazy_y[:,i,:]\n",
    "        elif which == 1:\n",
    "            this_patient_y = crazy_flags[:,i,:]\n",
    "        else:\n",
    "            this_patient_y = oned_flags[:,i,0:1]\n",
    "        for ii in range(crazy.shape[0]): #for each window of time sequence\n",
    "            this_patient_x_i = np.array([this_patient_x[ii,:,:]])\n",
    "            if which == 2:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,0:1]])\n",
    "            else:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,:]])\n",
    "            hists.append(model.train_on_batch(this_patient_x_i, this_patient_y_i))\n",
    "        model.reset_states()\n",
    "    return hists, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_stage2(train_split):\n",
    "    #second model: trading a stateful rnn using this batch thing\n",
    "    hists = []\n",
    "    model = Sequential()\n",
    "    \n",
    "    X  # this is our input data, of shape (32, 21, 16)\n",
    "    # we will feed it to our model in sequences of length 10\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, batch_input_shape=(32, 10, 16), stateful=True))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "    # we train the network to predict the 11th timestep given the first 10:\n",
    "    model.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))\n",
    "\n",
    "    # the state of the network has changed. We can feed the follow-up sequences:\n",
    "    model.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))\n",
    "\n",
    "    # let's reset the states of the LSTM layer:\n",
    "    model.reset_states()\n",
    "\n",
    "    # another way to do it in this case:\n",
    "    model.layers[0].reset_states()\n",
    "    \n",
    "    model.add(GRU(128,activation='relu', input_shape=(crazy.shape[2:])))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(train_split): # for each patient\n",
    "        this_patient_x = crazy[:,i,:,:]\n",
    "        if which == 0:\n",
    "            this_patient_y = crazy_y[:,i,:]\n",
    "        elif which == 1:\n",
    "            this_patient_y = crazy_flags[:,i,:]\n",
    "        else:\n",
    "            this_patient_y = oned_flags[:,i,0:1]\n",
    "        for ii in range(crazy.shape[0]): #for each window of time sequence\n",
    "            this_patient_x_i = np.array([this_patient_x[ii,:,:]])\n",
    "            if which == 2:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,0:1]])\n",
    "            else:\n",
    "                this_patient_y_i = np.array([this_patient_y[ii,:]])\n",
    "            hists.append(model.train_on_batch(this_patient_x_i, this_patient_y_i))\n",
    "        model.reset_states()\n",
    "    return hists, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oned_flags.shape\n",
    "(hists, model) = train_stage1(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compare the y between them\n",
    "print(oned_flags[:, 0, 0:1].shape)\n",
    "#this is y for normal stuff\n",
    "print(crazy_y[:,i,:][0,:].shape)\n",
    "#this is y for the newest one\n",
    "print((oned_flags[:,0,0:1][0,0:1]).shape)\n",
    "#compare the x between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(crazy[:, patient_num, :, :].shape)\n",
    "print((crazy[:,0,:,:][0,:,:]).shape)\n",
    "\n",
    "crazy[:,patient_num,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluate the model on the last 10 samples\n",
    "#which = 0 for the normal prediction\n",
    "#which = 1 for crazy_flags\n",
    "#which = 2 for oned_flags\n",
    "def evaluate(model, which):\n",
    "    evaluations = []\n",
    "    for i in range(n-10, n): # for each patient not in the training\n",
    "        this_patient_x = crazy[:,i,:,:]\n",
    "        if which == 0:\n",
    "            this_patient_y = crazy_y[:,i,:]\n",
    "        elif which == 1:\n",
    "            this_patient_y = crazy_flags[:,i,:]\n",
    "        else:\n",
    "            this_patient_y = oned_flags[:,i,:]\n",
    "        for ii in range(crazy.shape[0]): #for each window of time sequence\n",
    "            this_patient_x_i = np.array([this_patient_x[ii,:,:]])            \n",
    "            this_patient_y_i = np.array([this_patient_y[ii,:]])\n",
    "            evaluations.append(model.evaluate(this_patient_x_i, this_patient_y_i, verbose=0))\n",
    "        model.reset_states()\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = {}\n",
    "all_evals = {}\n",
    "which = 2\n",
    "for training_amount in range(10, 100, 10):\n",
    "    #Train the model\n",
    "    (hists, model) = train_stage1(training_amount, which)\n",
    "\n",
    "    #Check training results of the data\n",
    "    hist_values = []\n",
    "    for hist in hists:\n",
    "        hist_values.append(hist.item())\n",
    "    #plt.plot(hist_values)\n",
    "    histories[training_amount] = (sum(hist_values)/float(len(hist_values)))\n",
    "\n",
    "    #Check evaluation of the model\n",
    "    evaluations = evaluate(model, which)\n",
    "    #plt.plot(evaluations)\n",
    "    all_evals[training_amount] = (sum(evaluations)/float(len(evaluations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(histories)\n",
    "for key in histories:\n",
    "    print (key)\n",
    "    print (histories[key] - all_evals[key])\n",
    "print(all_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visually check the prediction of the first reading versus what's actually happening\n",
    "#(178, 200, 20, 10)\n",
    "\n",
    "#choose a patient\n",
    "patient_num = randint(101,199)\n",
    "#choose a measuremen\n",
    "measurement = 0\n",
    "\n",
    "#Get what we are aiming for\n",
    "if which == 0:\n",
    "    reality = crazy_y[:, patient_num, measurement]\n",
    "elif which == 1:\n",
    "    reality = crazy_flags[:, patient_num, measurement]\n",
    "elif which == 2:\n",
    "    reality = oned_flags[:, patient_num, 0:1]\n",
    "#Get the predictions\n",
    "model.reset_states()\n",
    "predictions = model.predict(crazy[:,patient_num,:,:])\n",
    "\n",
    "#Plot prediction versus reality\n",
    "plt.plot(predictions[:,0], color='red')\n",
    "plt.plot(reality, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute MSE\n",
    "dif = reality - predictions[:,0]\n",
    "dif = dif ** 2\n",
    "print (\"MSE:\")\n",
    "print (sum(dif)/178)\n",
    "\n",
    "#Plot the difference\n",
    "plt.plot(dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
